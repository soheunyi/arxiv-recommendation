[
  {
    "id": "2103.00112v3",
    "title": "Transformer in Transformer",
    "abstract": "Transformer is a new kind of neural architecture which encodes the input data as powerful features via the attention mechanism. Basically, the visual transformers first divide the input images into several local patches and then calculate both representations and their relationship. Since natural images are of high complexity with abundant detail and color information, the granularity of the patch dividing is not fine enough for excavating features of objects in different scales and locations. In this paper, we point out that the attention inside these local patches are also essential for building visual transformers with high performance and we explore a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we regard the local patches (e.g., 16$\\times$16) as \"visual sentences\" and present to further divide them into smaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each word will be calculated with other words in the given visual sentence with negligible computational costs. Features of both words and sentences will be aggregated to enhance the representation ability. Experiments on several benchmarks demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an 81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the state-of-the-art visual transformer with similar computational cost. The PyTorch code is available at https://github.com/huawei-noah/CV-Backbones, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/TNT.",
    "authors": [
      "Kai Han",
      "An Xiao",
      "Enhua Wu",
      "Jianyuan Guo",
      "Chunjing Xu",
      "Yunhe Wang"
    ],
    "category": "cs.CV",
    "published_date": "2021-02-27 03:12:16+00:00",
    "updated_date": "2021-10-26 02:24:24+00:00",
    "arxiv_url": "http://arxiv.org/abs/2103.00112v3",
    "pdf_url": "https://arxiv.org/pdf/2103.00112v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2307.01189v2",
    "title": "Trainable Transformer in Transformer",
    "abstract": "Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.",
    "authors": [
      "Abhishek Panigrahi",
      "Sadhika Malladi",
      "Mengzhou Xia",
      "Sanjeev Arora"
    ],
    "category": "cs.CL",
    "published_date": "2023-07-03 17:53:39+00:00",
    "updated_date": "2024-02-08 16:19:14+00:00",
    "arxiv_url": "http://arxiv.org/abs/2307.01189v2",
    "pdf_url": "https://arxiv.org/pdf/2307.01189v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "9801033v2",
    "title": "Quantum Transformations",
    "abstract": "We show that the stationary quantum Hamilton-Jacobi equation of non-relativistic 1D systems, underlying Bohmian mechanics, takes the classical form with $\\partial_q$ replaced by $\\partial_{\\hat q}$ where $d\\hat q={dq\\over \\sqrt{1-\\beta^2}}$. The $\\beta^2$ term essentially coincides with the quantum potential that, like $V-E$, turns out to be proportional to a curvature arising in projective geometry. In agreement with the recently formulated equivalence principle, these ``quantum transformations'' indicate that the classical and quantum potentials deform space geometry.",
    "authors": [
      "Alon E. Faraggi",
      "Marco Matone"
    ],
    "category": "hep-th",
    "published_date": "1998-01-09 00:50:38+00:00",
    "updated_date": "1998-09-17 14:19:39+00:00",
    "arxiv_url": "http://arxiv.org/abs/hep-th/9801033v2",
    "pdf_url": "https://arxiv.org/pdf/9801033v2.pdf",
    "doi": "http://dx.doi.org/10.1016/S0375-9601(98)00723-3",
    "journal_ref": "Phys.Lett.A249:180-190,1998",
    "topic_category": "transformer"
  },
  {
    "id": "0409265v1",
    "title": "Transformation Digroups",
    "abstract": "We introduce the notion of a transformation digroup and prove that every digroup is isomorphic to a transformation digroup.",
    "authors": [
      "Keqin Liu"
    ],
    "category": "math.GR",
    "published_date": "2004-09-16 16:32:13+00:00",
    "updated_date": "2004-09-16 16:32:13+00:00",
    "arxiv_url": "http://arxiv.org/abs/math/0409265v1",
    "pdf_url": "https://arxiv.org/pdf/0409265v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1011.3947v2",
    "title": "Covariant Transform",
    "abstract": "The paper develops theory of covariant transform, which is inspired by the wavelet construction. It was observed that many interesting types of wavelets (or coherent states) arise from group representations which are not square integrable or vacuum vectors which are not admissible. Covariant transform extends an applicability of the popular wavelets construction to classic examples like the Hardy space H_2, Banach spaces, covariant functional calculus and many others.   Keywords: Wavelets, coherent states, group representations, Hardy space, Littlewood-Paley operator, functional calculus, Berezin calculus, Radon transform, Moebius map, maximal function, affine group, special linear group, numerical range, characteristic function, functional model.",
    "authors": [
      "Vladimir V. Kisil"
    ],
    "category": "math.FA",
    "published_date": "2010-11-17 11:31:27+00:00",
    "updated_date": "2011-01-26 13:23:18+00:00",
    "arxiv_url": "http://arxiv.org/abs/1011.3947v2",
    "pdf_url": "https://arxiv.org/pdf/1011.3947v2.pdf",
    "doi": "http://dx.doi.org/10.1088/1742-6596/284/1/012038",
    "journal_ref": "J. Phys.: Conf. Ser. (2011), v. 284, p. 012038",
    "topic_category": "transformer"
  },
  {
    "id": "1103.0156v5",
    "title": "Lorentz Transformations",
    "abstract": "This paper describes a particularly didactic and transparent derivation of basic properties of the Lorentz group. The generators for rotations and boosts along an arbitrary direction, as well as their commutation relations, are written as functions of the unit vectors that define the axis of rotation or the direction of the boost (an approach that can be compared with the one that in electrodynamics, works with the electric and magnetic fields instead of the Maxwell stress tensor). For finite values of the angle of rotation or the boost's velocity, collectively denoted by V, the existence of an exponential expansion for the coordinate transformation's matrix, M (in terms of GV where G is the generator) requires that the matrix's derivative with respect to V, be equal to GM. This condition can only be satisfied if the transformation is additive as it is indeed the case for rotations, but not for velocities. If it is assumed, however, that for boosts such an expansion exists, with V = V(v), v being the velocity, and if the above condition is imposed on the boost's matrix then its expression in terms of hyperbolic cosh(V) and sinh(V} is recovered, and the expression for V(= arc tanh(v)) is determined. A general Lorentz transformation can be written as an exponential containing the sum of a rotation and a boost, which to first order is equal to the product of a boost with a rotation. The calculations of the second and third order terms show that the equations for the generators used in this paper, allow to reliably infer the expressions for the higher order generators, without having recourse to the commutation relations. The transformationmatrices for Weyl spinors are derived for finite values of the rotation and velocity, and field representations, leading to the expression for the angular momentum operator, are studied.",
    "authors": [
      "Bernard R. Durney"
    ],
    "category": "physics.gen-ph",
    "published_date": "2011-03-01 12:38:13+00:00",
    "updated_date": "2011-12-09 13:05:15+00:00",
    "arxiv_url": "http://arxiv.org/abs/1103.0156v5",
    "pdf_url": "https://arxiv.org/pdf/1103.0156v5.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1406.6512v1",
    "title": "Transforming magnets",
    "abstract": "Based on the form-invariant of Maxwell's equations under coordinate transformations, we extend the theory of transformation optics to transformation magneto-statics, which can design magnets through coordinate transformations. Some novel DC magnetic field illusions created by magnets (e.g. shirking magnets, cancelling magnets and overlapping magnets) are designed and verified by numerical simulations. Our research will open a new door to designing magnets and controlling DC magnetic fields.",
    "authors": [
      "F. Sun",
      "S. He"
    ],
    "category": "physics.class-ph",
    "published_date": "2014-06-25 09:59:26+00:00",
    "updated_date": "2014-06-25 09:59:26+00:00",
    "arxiv_url": "http://arxiv.org/abs/1406.6512v1",
    "pdf_url": "https://arxiv.org/pdf/1406.6512v1.pdf",
    "doi": "http://dx.doi.org/10.1038/srep06593",
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1510.05025v1",
    "title": "ADE Transform",
    "abstract": "There is a beautiful correspondence between configurations of lines on a rational surface and tautological bundles over that surface. We extend this correspondence to families, by means of a generalized Fourier-Mukai transform that relates spectral data to bundles over a rational surface fibration.",
    "authors": [
      "Ron Donagi",
      "Martijn Wijnholt"
    ],
    "category": "math.AG",
    "published_date": "2015-10-16 21:01:52+00:00",
    "updated_date": "2015-10-16 21:01:52+00:00",
    "arxiv_url": "http://arxiv.org/abs/1510.05025v1",
    "pdf_url": "https://arxiv.org/pdf/1510.05025v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1512.00795v2",
    "title": "Actions ~ Transformations",
    "abstract": "What defines an action like \"kicking ball\"? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.",
    "authors": [
      "Xiaolong Wang",
      "Ali Farhadi",
      "Abhinav Gupta"
    ],
    "category": "cs.CV",
    "published_date": "2015-12-02 18:17:32+00:00",
    "updated_date": "2016-07-26 04:51:49+00:00",
    "arxiv_url": "http://arxiv.org/abs/1512.00795v2",
    "pdf_url": "https://arxiv.org/pdf/1512.00795v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1608.03898v1",
    "title": "Curvature transformation",
    "abstract": "A transformation based on mean curvature is introduced which morphs triangulated surfaces into round spheres.",
    "authors": [
      "Dimitris Vartziotis"
    ],
    "category": "cs.GR",
    "published_date": "2016-07-22 12:52:09+00:00",
    "updated_date": "2016-07-22 12:52:09+00:00",
    "arxiv_url": "http://arxiv.org/abs/1608.03898v1",
    "pdf_url": "https://arxiv.org/pdf/1608.03898v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1701.02110v2",
    "title": "Transformation Forests",
    "abstract": "Regression models for supervised learning problems with a continuous target are commonly understood as models for the conditional mean of the target given predictors. This notion is simple and therefore appealing for interpretation and visualisation. Information about the whole underlying conditional distribution is, however, not available from these models. A more general understanding of regression models as models for conditional distributions allows much broader inference from such models, for example the computation of prediction intervals. Several random forest-type algorithms aim at estimating conditional distributions, most prominently quantile regression forests (Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric family of distributions characterised by their transformation function. A dedicated novel \"transformation tree\" algorithm able to detect distributional changes is developed. Based on these transformation trees, we introduce \"transformation forests\" as an adaptive local likelihood estimator of conditional distribution functions. The resulting models are fully parametric yet very general and allow broad inference procedures, such as the model-based bootstrap, to be applied in a straightforward way.",
    "authors": [
      "Torsten Hothorn",
      "Achim Zeileis"
    ],
    "category": "stat.ME",
    "published_date": "2017-01-09 09:52:03+00:00",
    "updated_date": "2018-01-08 10:08:16+00:00",
    "arxiv_url": "http://arxiv.org/abs/1701.02110v2",
    "pdf_url": "https://arxiv.org/pdf/1701.02110v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1802.05751v3",
    "title": "Image Transformer",
    "abstract": "Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.",
    "authors": [
      "Niki Parmar",
      "Ashish Vaswani",
      "Jakob Uszkoreit",
      "\u0141ukasz Kaiser",
      "Noam Shazeer",
      "Alexander Ku",
      "Dustin Tran"
    ],
    "category": "cs.CV",
    "published_date": "2018-02-15 20:37:15+00:00",
    "updated_date": "2018-06-15 23:27:07+00:00",
    "arxiv_url": "http://arxiv.org/abs/1802.05751v3",
    "pdf_url": "https://arxiv.org/pdf/1802.05751v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1809.04281v3",
    "title": "Music Transformer",
    "abstract": "Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.",
    "authors": [
      "Cheng-Zhi Anna Huang",
      "Ashish Vaswani",
      "Jakob Uszkoreit",
      "Noam Shazeer",
      "Ian Simon",
      "Curtis Hawthorne",
      "Andrew M. Dai",
      "Matthew D. Hoffman",
      "Monica Dinculescu",
      "Douglas Eck"
    ],
    "category": "cs.LG",
    "published_date": "2018-09-12 07:15:26+00:00",
    "updated_date": "2018-12-12 07:42:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/1809.04281v3",
    "pdf_url": "https://arxiv.org/pdf/1809.04281v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1902.09113v3",
    "title": "Star-Transformer",
    "abstract": "Although Transformer has achieved great successes on many NLP tasks, its heavy structure with fully-connected attention connections leads to dependencies on large training data. In this paper, we present Star-Transformer, a lightweight alternative by careful sparsification. To reduce model complexity, we replace the fully-connected structure with a star-shaped topology, in which every two non-adjacent nodes are connected through a shared relay node. Thus, complexity is reduced from quadratic to linear, while preserving capacity to capture both local composition and long-range dependency. The experiments on four tasks (22 datasets) show that Star-Transformer achieved significant improvements against the standard Transformer for the modestly sized datasets.",
    "authors": [
      "Qipeng Guo",
      "Xipeng Qiu",
      "Pengfei Liu",
      "Yunfan Shao",
      "Xiangyang Xue",
      "Zheng Zhang"
    ],
    "category": "cs.CL",
    "published_date": "2019-02-25 07:07:38+00:00",
    "updated_date": "2022-04-24 08:56:39+00:00",
    "arxiv_url": "http://arxiv.org/abs/1902.09113v3",
    "pdf_url": "https://arxiv.org/pdf/1902.09113v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1905.11006v2",
    "title": "Levenshtein Transformer",
    "abstract": "Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.",
    "authors": [
      "Jiatao Gu",
      "Changhan Wang",
      "Jake Zhao"
    ],
    "category": "cs.CL",
    "published_date": "2019-05-27 07:08:12+00:00",
    "updated_date": "2019-10-28 07:52:19+00:00",
    "arxiv_url": "http://arxiv.org/abs/1905.11006v2",
    "pdf_url": "https://arxiv.org/pdf/1905.11006v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2104.03964v1",
    "title": "Handwriting Transformers",
    "abstract": "We propose a novel transformer-based styled handwritten text image generation approach, HWT, that strives to learn both style-content entanglement as well as global and local writing style patterns. The proposed HWT captures the long and short range relationships within the style examples through a self-attention mechanism, thereby encoding both global and local style patterns. Further, the proposed transformer-based HWT comprises an encoder-decoder attention that enables style-content entanglement by gathering the style representation of each query character. To the best of our knowledge, we are the first to introduce a transformer-based generative network for styled handwritten text generation. Our proposed HWT generates realistic styled handwritten text images and significantly outperforms the state-of-the-art demonstrated through extensive qualitative, quantitative and human-based evaluations. The proposed HWT can handle arbitrary length of text and any desired writing style in a few-shot setting. Further, our HWT generalizes well to the challenging scenario where both words and writing style are unseen during training, generating realistic styled handwritten text images.",
    "authors": [
      "Ankan Kumar Bhunia",
      "Salman Khan",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan",
      "Mubarak Shah"
    ],
    "category": "cs.CV",
    "published_date": "2021-04-08 17:59:43+00:00",
    "updated_date": "2021-04-08 17:59:43+00:00",
    "arxiv_url": "http://arxiv.org/abs/2104.03964v1",
    "pdf_url": "https://arxiv.org/pdf/2104.03964v1.pdf",
    "doi": null,
    "journal_ref": "ICCV 2021",
    "topic_category": "transformer"
  },
  {
    "id": "2104.15031v1",
    "title": "Strongest transformations",
    "abstract": "We continue our study of maps transforming high-dimensional complicated objects into squares of stationary sets. Previously, we proved that many such transformations exist in ZFC, and here we address the consistency of the strongest conceivable transformations.   Along the way, we obtain new results on Shelah's coloring principle $Pr_1$. For $\\kappa$ inaccessible, we prove the consistency of $Pr_1(\\kappa,\\kappa,\\kappa,\\kappa)$. For successors of regulars, we obtain a full lifting of Galvin's 1980 theorem. In contrast, the full lifting of Galvin's theorem to successors of singulars is shown to be inconsistent.",
    "authors": [
      "Assaf Rinot",
      "Jing Zhang"
    ],
    "category": "math.LO",
    "published_date": "2021-04-30 14:38:28+00:00",
    "updated_date": "2021-04-30 14:38:28+00:00",
    "arxiv_url": "http://arxiv.org/abs/2104.15031v1",
    "pdf_url": "https://arxiv.org/pdf/2104.15031v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2105.00335v2",
    "title": "Audio Transformers",
    "abstract": "Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.",
    "authors": [
      "Prateek Verma",
      "Jonathan Berger"
    ],
    "category": "cs.SD",
    "published_date": "2021-05-01 19:38:30+00:00",
    "updated_date": "2025-05-11 23:57:58+00:00",
    "arxiv_url": "http://arxiv.org/abs/2105.00335v2",
    "pdf_url": "https://arxiv.org/pdf/2105.00335v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2203.08913v1",
    "title": "Memorizing Transformers",
    "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.",
    "authors": [
      "Yuhuai Wu",
      "Markus N. Rabe",
      "DeLesley Hutchins",
      "Christian Szegedy"
    ],
    "category": "cs.LG",
    "published_date": "2022-03-16 19:54:35+00:00",
    "updated_date": "2022-03-16 19:54:35+00:00",
    "arxiv_url": "http://arxiv.org/abs/2203.08913v1",
    "pdf_url": "https://arxiv.org/pdf/2203.08913v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2012.09164v2",
    "title": "Point Transformer",
    "abstract": "Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",
    "authors": [
      "Hengshuang Zhao",
      "Li Jiang",
      "Jiaya Jia",
      "Philip Torr",
      "Vladlen Koltun"
    ],
    "category": "cs.CV",
    "published_date": "2020-12-16 18:58:56+00:00",
    "updated_date": "2021-09-26 15:33:47+00:00",
    "arxiv_url": "http://arxiv.org/abs/2012.09164v2",
    "pdf_url": "https://arxiv.org/pdf/2012.09164v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2012.15045v2",
    "title": "Reservoir Transformers",
    "abstract": "We demonstrate that transformers obtain impressive performance even when some of the layers are randomly initialized and never updated. Inspired by old and well-established ideas in machine learning, we explore a variety of non-linear \"reservoir\" layers interspersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks.",
    "authors": [
      "Sheng Shen",
      "Alexei Baevski",
      "Ari S. Morcos",
      "Kurt Keutzer",
      "Michael Auli",
      "Douwe Kiela"
    ],
    "category": "cs.CL",
    "published_date": "2020-12-30 05:20:16+00:00",
    "updated_date": "2021-06-01 19:32:18+00:00",
    "arxiv_url": "http://arxiv.org/abs/2012.15045v2",
    "pdf_url": "https://arxiv.org/pdf/2012.15045v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2102.04432v2",
    "title": "Colorization Transformer",
    "abstract": "We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in more than 60% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available at https://github.com/google-research/google-research/tree/master/coltran",
    "authors": [
      "Manoj Kumar",
      "Dirk Weissenborn",
      "Nal Kalchbrenner"
    ],
    "category": "cs.CV",
    "published_date": "2021-02-08 18:45:06+00:00",
    "updated_date": "2021-03-07 08:38:49+00:00",
    "arxiv_url": "http://arxiv.org/abs/2102.04432v2",
    "pdf_url": "https://arxiv.org/pdf/2102.04432v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2103.15436v1",
    "title": "Transformer Tracking",
    "abstract": "Correlation acts as a critical role in the tracking field, especially in recent popular Siamese-based trackers. The correlation operation is a simple fusion manner to consider the similarity between the template and the search region. However, the correlation operation itself is a local linear matching process, leading to lose semantic information and fall into local optimum easily, which may be the bottleneck of designing high-accuracy tracking algorithms. Is there any better feature fusion method than correlation? To address this issue, inspired by Transformer, this work presents a novel attention-based feature fusion network, which effectively combines the template and search region features solely using attention. Specifically, the proposed method includes an ego-context augment module based on self-attention and a cross-feature augment module based on cross-attention. Finally, we present a Transformer tracking (named TransT) method based on the Siamese-like feature extraction backbone, the designed attention-based fusion mechanism, and the classification and regression head. Experiments show that our TransT achieves very promising results on six challenging datasets, especially on large-scale LaSOT, TrackingNet, and GOT-10k benchmarks. Our tracker runs at approximatively 50 fps on GPU. Code and models are available at https://github.com/chenxin-dlut/TransT.",
    "authors": [
      "Xin Chen",
      "Bin Yan",
      "Jiawen Zhu",
      "Dong Wang",
      "Xiaoyun Yang",
      "Huchuan Lu"
    ],
    "category": "cs.CV",
    "published_date": "2021-03-29 09:06:55+00:00",
    "updated_date": "2021-03-29 09:06:55+00:00",
    "arxiv_url": "http://arxiv.org/abs/2103.15436v1",
    "pdf_url": "https://arxiv.org/pdf/2103.15436v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2011.00931v2",
    "title": "Point Transformer",
    "abstract": "In this work, we present Point Transformer, a deep neural network that operates directly on unordered and unstructured point sets. We design Point Transformer to extract local and global features and relate both representations by introducing the local-global attention mechanism, which aims to capture spatial point relations and shape information. For that purpose, we propose SortNet, as part of the Point Transformer, which induces input permutation invariance by selecting points based on a learned score. The output of Point Transformer is a sorted and permutation invariant feature list that can directly be incorporated into common computer vision applications. We evaluate our approach on standard classification and part segmentation benchmarks to demonstrate competitive results compared to the prior work. Code is publicly available at: https://github.com/engelnico/point-transformer",
    "authors": [
      "Nico Engel",
      "Vasileios Belagiannis",
      "Klaus Dietmayer"
    ],
    "category": "cs.CV",
    "published_date": "2020-11-02 12:26:14+00:00",
    "updated_date": "2021-10-14 10:51:39+00:00",
    "arxiv_url": "http://arxiv.org/abs/2011.00931v2",
    "pdf_url": "https://arxiv.org/pdf/2011.00931v2.pdf",
    "doi": "http://dx.doi.org/10.1109/ACCESS.2021.3116304",
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2202.04942v2",
    "title": "Spherical Transformer",
    "abstract": "Using convolutional neural networks for 360images can induce sub-optimal performance due to distortions entailed by a planar projection. The distortion gets deteriorated when a rotation is applied to the 360image. Thus, many researches based on convolutions attempt to reduce the distortions to learn accurate representation. In contrast, we leverage the transformer architecture to solve image classification problems for 360images. Using the proposed transformer for 360images has two advantages. First, our method does not require the erroneous planar projection process by sampling pixels from the sphere surface. Second, our sampling method based on regular polyhedrons makes low rotation equivariance errors, because specific rotations can be reduced to permutations of faces. In experiments, we validate our network on two aspects, as follows. First, we show that using a transformer with highly uniform sampling methods can help reduce the distortion. Second, we demonstrate that the transformer architecture can achieve rotation equivariance on specific rotations. We compare our method to other state-of-the-art algorithms using the SPH-MNIST, SPH-CIFAR, and SUN360 datasets and show that our method is competitive with other methods.",
    "authors": [
      "Sungmin Cho",
      "Raehyuk Jung",
      "Junseok Kwon"
    ],
    "category": "cs.CV",
    "published_date": "2022-02-10 10:24:24+00:00",
    "updated_date": "2022-02-11 07:29:03+00:00",
    "arxiv_url": "http://arxiv.org/abs/2202.04942v2",
    "pdf_url": "https://arxiv.org/pdf/2202.04942v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1807.03819v3",
    "title": "Universal Transformers",
    "abstract": "Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.",
    "authors": [
      "Mostafa Dehghani",
      "Stephan Gouws",
      "Oriol Vinyals",
      "Jakob Uszkoreit",
      "\u0141ukasz Kaiser"
    ],
    "category": "cs.CL",
    "published_date": "2018-07-10 18:39:15+00:00",
    "updated_date": "2019-03-05 16:46:19+00:00",
    "arxiv_url": "http://arxiv.org/abs/1807.03819v3",
    "pdf_url": "https://arxiv.org/pdf/1807.03819v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1910.00486v3",
    "title": "Dialogue Transformers",
    "abstract": "We introduce a dialogue policy based on a transformer architecture, where the self-attention mechanism operates over the sequence of dialogue turns. Recent work has used hierarchical recurrent neural networks to encode multiple utterances in a dialogue context, but we argue that a pure self-attention mechanism is more suitable. By default, an RNN assumes that every item in a sequence is relevant for producing an encoding of the full sequence, but a single conversation can consist of multiple overlapping discourse segments as speakers interleave multiple topics. A transformer picks which turns to include in its encoding of the current dialogue state, and is naturally suited to selectively ignoring or attending to dialogue history. We compare the performance of the Transformer Embedding Dialogue (TED) policy to an LSTM and to the REDP, which was specifically designed to overcome this limitation of RNNs.",
    "authors": [
      "Vladimir Vlasov",
      "Johannes E. M. Mosig",
      "Alan Nichol"
    ],
    "category": "cs.CL",
    "published_date": "2019-10-01 15:36:27+00:00",
    "updated_date": "2020-05-01 07:43:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/1910.00486v3",
    "pdf_url": "https://arxiv.org/pdf/1910.00486v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2005.04551v1",
    "title": "Epipolar Transformers",
    "abstract": "A common approach to localize 3D human joints in a synchronized and calibrated multi-view setup consists of two-steps: (1) apply a 2D detector separately on each view to localize joints in 2D, and (2) perform robust triangulation on 2D detections from each view to acquire the 3D joint locations. However, in step 1, the 2D detector is limited to solving challenging cases which could potentially be better resolved in 3D, such as occlusions and oblique viewing angles, purely in 2D without leveraging any 3D information. Therefore, we propose the differentiable \"epipolar transformer\", which enables the 2D detector to leverage 3D-aware features to improve 2D pose estimation. The intuition is: given a 2D location p in the current view, we would like to first find its corresponding point p' in a neighboring view, and then combine the features at p' with the features at p, thus leading to a 3D-aware feature at p. Inspired by stereo matching, the epipolar transformer leverages epipolar constraints and feature matching to approximate the features at p'. Experiments on InterHand and Human3.6M show that our approach has consistent improvements over the baselines. Specifically, in the condition where no external data is used, our Human3.6M model trained with ResNet-50 backbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and achieves MPJPE 26.9 mm.",
    "authors": [
      "Yihui He",
      "Rui Yan",
      "Katerina Fragkiadaki",
      "Shoou-I Yu"
    ],
    "category": "cs.CV",
    "published_date": "2020-05-10 02:22:54+00:00",
    "updated_date": "2020-05-10 02:22:54+00:00",
    "arxiv_url": "http://arxiv.org/abs/2005.04551v1",
    "pdf_url": "https://arxiv.org/pdf/2005.04551v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2006.11527v2",
    "title": "Memory Transformer",
    "abstract": "Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model's ability to process a global context.",
    "authors": [
      "Mikhail S. Burtsev",
      "Yuri Kuratov",
      "Anton Peganov",
      "Grigory V. Sapunov"
    ],
    "category": "cs.CL",
    "published_date": "2020-06-20 09:06:27+00:00",
    "updated_date": "2021-02-16 08:06:47+00:00",
    "arxiv_url": "http://arxiv.org/abs/2006.11527v2",
    "pdf_url": "https://arxiv.org/pdf/2006.11527v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2010.15583v3",
    "title": "Probabilistic Transformers",
    "abstract": "We show that Transformers are Maximum Posterior Probability estimators for Mixtures of Gaussian Models. This brings a probabilistic point of view to Transformers and suggests extensions to other probabilistic cases.",
    "authors": [
      "Javier R. Movellan",
      "Prasad Gabbur"
    ],
    "category": "cs.LG",
    "published_date": "2020-10-15 01:44:59+00:00",
    "updated_date": "2020-11-12 16:40:17+00:00",
    "arxiv_url": "http://arxiv.org/abs/2010.15583v3",
    "pdf_url": "https://arxiv.org/pdf/2010.15583v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2204.05172v2",
    "title": "Event Transformer",
    "abstract": "The event camera's low power consumption and ability to capture microsecond brightness changes make it attractive for various computer vision tasks. Existing event representation methods typically convert events into frames, voxel grids, or spikes for deep neural networks (DNNs). However, these approaches often sacrifice temporal granularity or require specialized devices for processing. This work introduces a novel token-based event representation, where each event is considered a fundamental processing unit termed an event-token. This approach preserves the sequence's intricate spatiotemporal attributes at the event level. Moreover, we propose a Three-way Attention mechanism in the Event Transformer Block (ETB) to collaboratively construct temporal and spatial correlations between events. We compare our proposed token-based event representation extensively with other prevalent methods for object classification and optical flow estimation. The experimental results showcase its competitive performance while demanding minimal computational resources on standard devices. Our code is publicly accessible at \\url{https://github.com/NJUVISION/EventTransformer}.",
    "authors": [
      "Bin Jiang",
      "Zhihao Li",
      "M. Salman Asif",
      "Xun Cao",
      "Zhan Ma"
    ],
    "category": "cs.CV",
    "published_date": "2022-04-11 15:05:06+00:00",
    "updated_date": "2024-06-12 15:06:10+00:00",
    "arxiv_url": "http://arxiv.org/abs/2204.05172v2",
    "pdf_url": "https://arxiv.org/pdf/2204.05172v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2205.12956v2",
    "title": "Inception Transformer",
    "abstract": "Recent studies show that Transformer has strong capability of building long-range dependencies, yet is incompetent in capturing high frequencies that predominantly convey local information. To tackle this issue, we present a novel and general-purpose Inception Transformer, or iFormer for short, that effectively learns comprehensive features with both high- and low-frequency information in visual data. Specifically, we design an Inception mixer to explicitly graft the advantages of convolution and max-pooling for capturing the high-frequency information to Transformers. Different from recent hybrid frameworks, the Inception mixer brings greater efficiency through a channel splitting mechanism to adopt parallel convolution/max-pooling path and self-attention path as high- and low-frequency mixers, while having the flexibility to model discriminative information scattered within a wide frequency range. Considering that bottom layers play more roles in capturing high-frequency details while top layers more in modeling low-frequency global information, we further introduce a frequency ramp structure, i.e. gradually decreasing the dimensions fed to the high-frequency mixer and increasing those to the low-frequency mixer, which can effectively trade-off high- and low-frequency components across different layers. We benchmark the iFormer on a series of vision tasks, and showcase that it achieves impressive performance on image classification, COCO detection and ADE20K segmentation. For example, our iFormer-S hits the top-1 accuracy of 83.4% on ImageNet-1K, much higher than DeiT-S by 3.6%, and even slightly better than much bigger model Swin-B (83.3%) with only 1/4 parameters and 1/3 FLOPs. Code and models will be released at https://github.com/sail-sg/iFormer.",
    "authors": [
      "Chenyang Si",
      "Weihao Yu",
      "Pan Zhou",
      "Yichen Zhou",
      "Xinchao Wang",
      "Shuicheng Yan"
    ],
    "category": "cs.CV",
    "published_date": "2022-05-25 17:59:54+00:00",
    "updated_date": "2022-05-26 17:18:32+00:00",
    "arxiv_url": "http://arxiv.org/abs/2205.12956v2",
    "pdf_url": "https://arxiv.org/pdf/2205.12956v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2210.06423v2",
    "title": "Foundation Transformers",
    "abstract": "A big convergence of model architectures across language, vision, speech, and multimodal is emerging. However, under the same name \"Transformers\", the above areas use different implementations for better performance, e.g., Post-LayerNorm for BERT, and Pre-LayerNorm for GPT and vision Transformers. We call for the development of Foundation Transformer for true general-purpose modeling, which serves as a go-to architecture for various tasks and modalities with guaranteed training stability. In this work, we introduce a Transformer variant, named Magneto, to fulfill the goal. Specifically, we propose Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up. Extensive experiments demonstrate its superior performance and better stability than the de facto Transformer variants designed for various applications, including language modeling (i.e., BERT, and GPT), machine translation, vision pretraining (i.e., BEiT), speech recognition, and multimodal pretraining (i.e., BEiT-3).",
    "authors": [
      "Hongyu Wang",
      "Shuming Ma",
      "Shaohan Huang",
      "Li Dong",
      "Wenhui Wang",
      "Zhiliang Peng",
      "Yu Wu",
      "Payal Bajaj",
      "Saksham Singhal",
      "Alon Benhaim",
      "Barun Patra",
      "Zhun Liu",
      "Vishrav Chaudhary",
      "Xia Song",
      "Furu Wei"
    ],
    "category": "cs.LG",
    "published_date": "2022-10-12 17:16:27+00:00",
    "updated_date": "2022-10-19 11:03:35+00:00",
    "arxiv_url": "http://arxiv.org/abs/2210.06423v2",
    "pdf_url": "https://arxiv.org/pdf/2210.06423v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2302.07253v2",
    "title": "Energy Transformer",
    "abstract": "Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion task, and obtain strong quantitative results on the graph anomaly detection and graph classification tasks.",
    "authors": [
      "Benjamin Hoover",
      "Yuchen Liang",
      "Bao Pham",
      "Rameswar Panda",
      "Hendrik Strobelt",
      "Duen Horng Chau",
      "Mohammed J. Zaki",
      "Dmitry Krotov"
    ],
    "category": "cs.LG",
    "published_date": "2023-02-14 18:51:22+00:00",
    "updated_date": "2023-11-01 00:14:30+00:00",
    "arxiv_url": "http://arxiv.org/abs/2302.07253v2",
    "pdf_url": "https://arxiv.org/pdf/2302.07253v2.pdf",
    "doi": null,
    "journal_ref": "37th Conference on Neural Information Processing Systems (NeurIPS\n  2023)",
    "topic_category": "transformer"
  },
  {
    "id": "2302.10360v1",
    "title": "Optical Transformers",
    "abstract": "The rapidly increasing size of deep-learning models has caused renewed and growing interest in alternatives to digital computers to dramatically reduce the energy cost of running state-of-the-art neural networks. Optical matrix-vector multipliers are best suited to performing computations with very large operands, which suggests that large Transformer models could be a good target for optical computing. To test this idea, we performed small-scale optical experiments with a prototype accelerator to demonstrate that Transformer operations can run on optical hardware despite noise and errors. Using simulations, validated by our experiments, we then explored the energy efficiency of optical implementations of Transformers and identified scaling laws for model performance with respect to optical energy usage. We found that the optical energy per multiply-accumulate (MAC) scales as $\\frac{1}{d}$ where $d$ is the Transformer width, an asymptotic advantage over digital systems. We conclude that with well-engineered, large-scale optical hardware, it may be possible to achieve a $100 \\times$ energy-efficiency advantage for running some of the largest current Transformer models, and that if both the models and the optical hardware are scaled to the quadrillion-parameter regime, optical computers could have a $>8,000\\times$ energy-efficiency advantage over state-of-the-art digital-electronic processors that achieve 300 fJ/MAC. We analyzed how these results motivate and inform the construction of future optical accelerators along with optics-amenable deep-learning approaches. With assumptions about future improvements to electronics and Transformer quantization techniques (5$\\times$ cheaper memory access, double the digital--analog conversion efficiency, and 4-bit precision), we estimated that optical computers' advantage against current 300-fJ/MAC digital processors could grow to $>100,000\\times$.",
    "authors": [
      "Maxwell G. Anderson",
      "Shi-Yuan Ma",
      "Tianyu Wang",
      "Logan G. Wright",
      "Peter L. McMahon"
    ],
    "category": "cs.ET",
    "published_date": "2023-02-20 23:30:23+00:00",
    "updated_date": "2023-02-20 23:30:23+00:00",
    "arxiv_url": "http://arxiv.org/abs/2302.10360v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10360v1.pdf",
    "doi": null,
    "journal_ref": "Transactions on Machine Learning Research, 03/2024,\n  https://openreview.net/forum?id=Xxw0edFFQC",
    "topic_category": "transformer"
  },
  {
    "id": "2309.12862v4",
    "title": "Associative Transformer",
    "abstract": "Emerging from the pairwise attention in conventional Transformers, there is a growing interest in sparse attention mechanisms that align more closely with localized, contextual learning in the biological brain. Existing studies such as the Coordination method employ iterative cross-attention mechanisms with a bottleneck to enable the sparse association of inputs. However, these methods are parameter inefficient and fail in more complex relational reasoning tasks. To this end, we propose Associative Transformer (AiT) to enhance the association among sparsely attended input tokens, improving parameter efficiency and performance in various vision tasks such as classification and relational reasoning. AiT leverages a learnable explicit memory comprising specialized priors that guide bottleneck attentions to facilitate the extraction of diverse localized tokens. Moreover, AiT employs an associative memory-based token reconstruction using a Hopfield energy function. The extensive empirical experiments demonstrate that AiT requires significantly fewer parameters and attention layers outperforming a broad range of sparse Transformer models. Additionally, AiT outperforms the SOTA sparse Transformer models including the Coordination method on the Sort-of-CLEVR dataset.",
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai",
      "Zhirong Wu",
      "Stephen Lin",
      "Ryota Kanai"
    ],
    "category": "cs.LG",
    "published_date": "2023-09-22 13:37:10+00:00",
    "updated_date": "2025-03-11 09:04:22+00:00",
    "arxiv_url": "http://arxiv.org/abs/2309.12862v4",
    "pdf_url": "https://arxiv.org/pdf/2309.12862v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2311.01724v4",
    "title": "Holography Transformer",
    "abstract": "We have constructed a generative artificial intelligence model to predict dual gravity solutions when provided with the input of holographic entanglement entropy. The model utilized in our study is based on the transformer algorithm, widely used for various natural language tasks including text generation, summarization, and translation. This algorithm possesses the ability to understand the meanings of input and output sequences by utilizing multi-head attention layers. In the training procedure, we generated pairs of examples consisting of holographic entanglement entropy data and their corresponding metric solutions. Once the model has completed the training process, it demonstrates the ability to generate predictions regarding a dual geometry that corresponds to the given holographic entanglement entropy. Subsequently, we proceed to validate the dual geometry to confirm its correspondence with the holographic entanglement entropy data.",
    "authors": [
      "Chanyong Park",
      "Sejin Kim",
      "Jung Hun Lee"
    ],
    "category": "hep-th",
    "published_date": "2023-11-03 05:41:49+00:00",
    "updated_date": "2025-07-02 13:22:05+00:00",
    "arxiv_url": "http://arxiv.org/abs/2311.01724v4",
    "pdf_url": "https://arxiv.org/pdf/2311.01724v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2311.03235v1",
    "title": "p-Laplacian Transformer",
    "abstract": "$p$-Laplacian regularization, rooted in graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.",
    "authors": [
      "Tuan Nguyen",
      "Tam Nguyen",
      "Vinh Nguyen",
      "Tan M. Nguyen"
    ],
    "category": "cs.LG",
    "published_date": "2023-11-06 16:25:56+00:00",
    "updated_date": "2023-11-06 16:25:56+00:00",
    "arxiv_url": "http://arxiv.org/abs/2311.03235v1",
    "pdf_url": "https://arxiv.org/pdf/2311.03235v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2410.05258v2",
    "title": "Differential Transformer",
    "abstract": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Yuqing Xia",
      "Yutao Sun",
      "Yi Zhu",
      "Gao Huang",
      "Furu Wei"
    ],
    "category": "cs.CL",
    "published_date": "2024-10-07 17:57:38+00:00",
    "updated_date": "2025-04-07 12:04:28+00:00",
    "arxiv_url": "http://arxiv.org/abs/2410.05258v2",
    "pdf_url": "https://arxiv.org/pdf/2410.05258v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2501.17486v1",
    "title": "DINT Transformer",
    "abstract": "DIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.",
    "authors": [
      "Yueyang Cang",
      "Yuhang Liu",
      "Xiaoteng Zhang",
      "Erlu Zhao",
      "Li Shi"
    ],
    "category": "cs.CL",
    "published_date": "2025-01-29 08:53:29+00:00",
    "updated_date": "2025-01-29 08:53:29+00:00",
    "arxiv_url": "http://arxiv.org/abs/2501.17486v1",
    "pdf_url": "https://arxiv.org/pdf/2501.17486v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "9304214v1",
    "title": "Wavelet transforms versus Fourier transforms",
    "abstract": "This note is a very basic introduction to wavelets. It starts with an orthogonal basis of piecewise constant functions, constructed by dilation and translation. The ``wavelet transform'' maps each $f(x)$ to its coefficients with respect to this basis. The mathematics is simple and the transform is fast (faster than the Fast Fourier Transform, which we briefly explain), but approximation by piecewise constants is poor. To improve this first wavelet, we are led to dilation equations and their unusual solutions. Higher-order wavelets are constructed, and it is surprisingly quick to compute with them --- always indirectly and recursively. We comment informally on the contest between these transforms in signal processing, especially for video and image compression (including high-definition television). So far the Fourier Transform --- or its 8 by 8 windowed version, the Discrete Cosine Transform --- is often chosen. But wavelets are already competitive, and they are ahead for fingerprints. We present a sample of this developing theory.",
    "authors": [
      "Gilbert Strang"
    ],
    "category": "math.NA",
    "published_date": "1993-04-01 00:00:00+00:00",
    "updated_date": "1993-04-01 00:00:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/math/9304214v1",
    "pdf_url": "https://arxiv.org/pdf/9304214v1.pdf",
    "doi": null,
    "journal_ref": "Bull. Amer. Math. Soc. (N.S.) 28 (1993) 288-305",
    "topic_category": "transformer"
  },
  {
    "id": "0911.2162v1",
    "title": "Integral transformation and Darboux transformation",
    "abstract": "We review Darboux-Crum transformation of Heun's differential equation. By rewriting an integral transformation of Heun's differential equation into a form of elliptic functions, we see that the integral representation is a generalization of Darboux-Crum transformation. We also consider conservation of monodromy with respect to the transformations.",
    "authors": [
      "Kouichi Takemura"
    ],
    "category": "math.CA",
    "published_date": "2009-11-11 15:39:29+00:00",
    "updated_date": "2009-11-11 15:39:29+00:00",
    "arxiv_url": "http://arxiv.org/abs/0911.2162v1",
    "pdf_url": "https://arxiv.org/pdf/0911.2162v1.pdf",
    "doi": "http://dx.doi.org/10.1063/1.3367079",
    "journal_ref": "AIP Conf.Proc.1212:58-65,2010",
    "topic_category": "transformer"
  },
  {
    "id": "1105.1427v2",
    "title": "Riesz transforms for Dunkl transform",
    "abstract": "In this paper we obtain the $L^p$-boundedness of Riesz transforms for Dunkl transform for all $1<p<\\infty$.",
    "authors": [
      "B\u00e9chir Amri",
      "Mohamed Sifi"
    ],
    "category": "math.CA",
    "published_date": "2011-05-07 09:00:11+00:00",
    "updated_date": "2011-05-12 14:49:30+00:00",
    "arxiv_url": "http://arxiv.org/abs/1105.1427v2",
    "pdf_url": "https://arxiv.org/pdf/1105.1427v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1107.3625v1",
    "title": "Appell Transformation and Canonical Transforms",
    "abstract": "The interpretation of the optical Appell transformation, as previously elaborated in relation to the free-space paraxial propagation under both a rectangular and a circular cylindrical symmetry, is reviewed. Then, the caloric Appell transformation, well known in the theory of heat equation, is shown to be amenable for a similar interpretation involving the Laplace transform rather than the Fourier transform, when dealing with the 1D heat equation. Accordingly, when considering the radial heat equation, suitably defined Hankel-type transforms come to be involved in the inherent Appell transformation. The analysis is aimed at outlining the link between the Appell transformation and the canonical transforms.",
    "authors": [
      "Amalia Torre"
    ],
    "category": "math-ph",
    "published_date": "2011-07-19 05:25:36+00:00",
    "updated_date": "2011-07-19 05:25:36+00:00",
    "arxiv_url": "http://arxiv.org/abs/1107.3625v1",
    "pdf_url": "https://arxiv.org/pdf/1107.3625v1.pdf",
    "doi": "http://dx.doi.org/10.3842/SIGMA.2011.072",
    "journal_ref": "SIGMA 7 (2011), 072, 34 pages",
    "topic_category": "transformer"
  },
  {
    "id": "2111.07602v1",
    "title": "Spectral Transform Forms Scalable Transformer",
    "abstract": "Many real-world relational systems, such as social networks and biological systems, contain dynamic interactions. When learning dynamic graph representation, it is essential to employ sequential temporal information and geometric structure. Mainstream work achieves topological embedding via message passing networks (e.g., GCN, GAT). The temporal evolution, on the other hand, is conventionally expressed via memory units (e.g., LSTM or GRU) that possess convenient information filtration in a gate mechanism. Though, such a design prevents large-scale input sequence due to the over-complicated encoding. This work learns from the philosophy of self-attention and proposes an efficient spectral-based neural unit that employs informative long-range temporal interaction. The developed spectral window unit (SWINIT) model predicts scalable dynamic graphs with assured efficiency. The architecture is assembled with a few simple effective computational blocks that constitute randomized SVD, MLP, and graph Framelet convolution. The SVD plus MLP module encodes the long-short-term feature evolution of the dynamic graph events. A fast framelet graph transform in the framelet convolution embeds the structural dynamics. Both strategies enhance the model's ability on scalable analysis. In particular, the iterative SVD approximation shrinks the computational complexity of attention to O(Nd\\log(d)) for the dynamic graph with N edges and d edge features, and the multiscale transform of framelet convolution allows sufficient scalability in the network training. Our SWINIT achieves state-of-the-art performance on a variety of online continuous-time dynamic graph learning tasks, while compared to baseline methods, the number of its learnable parameters reduces by up to seven times.",
    "authors": [
      "Bingxin Zhou",
      "Xinliang Liu",
      "Yuehua Liu",
      "Yunying Huang",
      "Pietro Li\u00f2",
      "YuGuang Wang"
    ],
    "category": "cs.LG",
    "published_date": "2021-11-15 08:46:01+00:00",
    "updated_date": "2021-11-15 08:46:01+00:00",
    "arxiv_url": "http://arxiv.org/abs/2111.07602v1",
    "pdf_url": "https://arxiv.org/pdf/2111.07602v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "0511211v1",
    "title": "Gauge transformations are not canonical transformations",
    "abstract": "In classical mechanics, we can describe the dynamics of a given system using either the Lagrangian formalism or the Hamiltonian formalism, the choice of either one being determined by whether one wants to deal with a second degree differential equation or a pair of first degree ones. For the former approach, we know that the Euler-Lagrange equation of motion remains invariant under additive total derivative with respect to time of any function of coordinates and time in the Lagrangian function, whereas the latter one is invariant under canonical transformations. In this short paper we address the question whether the transformation that leaves the Euler-Lagrange equation of motion invariant is also a canonical transformation and show that it is not.",
    "authors": [
      "A. T. Suzuki",
      "J. H. O. Sales"
    ],
    "category": "hep-th",
    "published_date": "2005-11-21 18:20:51+00:00",
    "updated_date": "2005-11-21 18:20:51+00:00",
    "arxiv_url": "http://arxiv.org/abs/hep-th/0511211v1",
    "pdf_url": "https://arxiv.org/pdf/0511211v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "0604173v1",
    "title": "Comment on \"Gauge transformations are Canonical transformations\"",
    "abstract": "We comment on the work of Tai L Chow, Eur. J. Phys. 18, 467 (1997). By considering the Lagrangians which are uniquely defined only to within an additive total time derivative of a function of co-ordinates and time the author has tried to show that the gauge transformations which relate these Lagrangians are canonical transformations. He has obtained the right conclusion only by using wrong canonical equations and the entire exercise has hence become erroneous and inconclusive. By using the definition of canonical transformation through Poisson brackets we prove that the above gauge transformations are canonical transformations.",
    "authors": [
      "Pathikrit Bhattacharya",
      "Bhabani Prasad Mandal"
    ],
    "category": "physics.class-ph",
    "published_date": "2006-04-21 04:06:28+00:00",
    "updated_date": "2006-04-21 04:06:28+00:00",
    "arxiv_url": "http://arxiv.org/abs/physics/0604173v1",
    "pdf_url": "https://arxiv.org/pdf/0604173v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "0704.2744v2",
    "title": "Nahm transform and parabolic minimal Laplace transform",
    "abstract": "We prove that Nahm transform for integrable connections with a finite number of regular singularities and an irregular singularity of rank 1 on the Riemann sphere is equivalent -- up to considering integrable connections as holonomic $\\D$-modules -- to minimal Laplace transform. We assume semi-simplicity and resonance-freeness conditions, and we work in the framework of objects with a parabolic structure. In particular, we describe the definition of the parabolic version of Laplace transform due to C. Sabbah. The proof of the main result relies on the study of a twisted de Rham complex.",
    "authors": [
      "Szilard Szabo"
    ],
    "category": "math.AG",
    "published_date": "2007-04-20 15:00:53+00:00",
    "updated_date": "2011-09-02 15:32:04+00:00",
    "arxiv_url": "http://arxiv.org/abs/0704.2744v2",
    "pdf_url": "https://arxiv.org/pdf/0704.2744v2.pdf",
    "doi": null,
    "journal_ref": "Journal of Geometry and Physics 62 (2012) 2241--2258",
    "topic_category": "transformer"
  },
  {
    "id": "0707.2338v3",
    "title": "Lorentz transformation by mimicking the Lorentz transformation",
    "abstract": "We show that starting with the fact that special relativity theory is concerned with a distortion of the observed length of a moving rod, without mentioning if it is a \"contraction\" or \"dilation\", we can derive the Lorentz transformations for the spacetime coordinates of the same event. This derivation is based on expressing the length of the moving rod as a sum of components with all the lengths involved in this summation being measured by the observers of the same inertial reference frame.",
    "authors": [
      "Bernhard Rothenstein",
      "Stefan Popescu"
    ],
    "category": "physics.gen-ph",
    "published_date": "2007-07-16 15:09:38+00:00",
    "updated_date": "2007-09-24 12:30:47+00:00",
    "arxiv_url": "http://arxiv.org/abs/0707.2338v3",
    "pdf_url": "https://arxiv.org/pdf/0707.2338v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1110.1589v2",
    "title": "Type-II B\u00e4cklund Transformations via Gauge Transformations",
    "abstract": "The construction of type II Backlund transformation for the sine-Gordon and the Tzitzeica-Bullough-Dodd models are obtained from gauge transformation. An infinite number of conserved quantities are constructed from the defect matrices. This guarantees that the introduction of type II defects for these models does not spoil their integrability. In particular, modified energy and momentum are derived and compared with those presented in recent literature.",
    "authors": [
      "A. R. Aguirre",
      "T. R. Araujo",
      "J. F. Gomes",
      "A. H. Zimerman"
    ],
    "category": "nlin.SI",
    "published_date": "2011-10-07 17:19:27+00:00",
    "updated_date": "2011-12-19 13:22:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/1110.1589v2",
    "pdf_url": "https://arxiv.org/pdf/1110.1589v2.pdf",
    "doi": "http://dx.doi.org/10.1007/JHEP12(2011)056",
    "journal_ref": "Journal of High Energy Physics, Volume 2011, Number 12, 56",
    "topic_category": "transformer"
  },
  {
    "id": "2109.15129v1",
    "title": "Convolution-Free Waveform Transformers for Multi-Lead ECG Classification",
    "abstract": "We present our entry to the 2021 PhysioNet/CinC challenge - a waveform transformer model to detect cardiac abnormalities from ECG recordings. We compare the performance of the waveform transformer model on different ECG-lead subsets using approximately 88,000 ECG recordings from six datasets. In the official rankings, team prna ranked between 9 and 15 on 12, 6, 4, 3 and 2-lead sets respectively. Our waveform transformer model achieved an average challenge metric of 0.47 on the held-out test set across all ECG-lead subsets. Our combined performance across all leads placed us at rank 11 out of 39 officially ranking teams.",
    "authors": [
      "Annamalai Natarajan",
      "Gregory Boverman",
      "Yale Chang",
      "Corneliu Antonescu",
      "Jonathan Rubin"
    ],
    "category": "eess.SP",
    "published_date": "2021-09-29 12:54:15+00:00",
    "updated_date": "2021-09-29 12:54:15+00:00",
    "arxiv_url": "http://arxiv.org/abs/2109.15129v1",
    "pdf_url": "https://arxiv.org/pdf/2109.15129v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2110.15225v3",
    "title": "Pruning Attention Heads of Transformer Models Using A* Search: A Novel   Approach to Compress Big NLP Architectures",
    "abstract": "Recent years have seen a growing adoption of Transformer models such as BERT in Natural Language Processing and even in Computer Vision. However, due to their size, there has been limited adoption of such models within resource-constrained computing environments. This paper proposes novel pruning algorithm to compress transformer models by eliminating redundant Attention Heads. We apply the A* search algorithm to obtain a pruned model with strict accuracy guarantees. Our results indicate that the method could eliminate as much as 40% of the attention heads in the BERT transformer model with no loss in accuracy.",
    "authors": [
      "Archit Parnami",
      "Rahul Singh",
      "Tarun Joshi"
    ],
    "category": "cs.CL",
    "published_date": "2021-10-28 15:39:11+00:00",
    "updated_date": "2021-11-17 14:50:51+00:00",
    "arxiv_url": "http://arxiv.org/abs/2110.15225v3",
    "pdf_url": "https://arxiv.org/pdf/2110.15225v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2211.02519v1",
    "title": "BERT for Long Documents: A Case Study of Automated ICD Coding",
    "abstract": "Transformer models have achieved great success across many NLP problems. However, previous studies in automated ICD coding concluded that these models fail to outperform some of the earlier solutions such as CNN-based models. In this paper we challenge this conclusion. We present a simple and scalable method to process long text with the existing transformer models such as BERT. We show that this method significantly improves the previous results reported for transformer models in ICD coding, and is able to outperform one of the prominent CNN-based methods.",
    "authors": [
      "Arash Afkanpour",
      "Shabir Adeel",
      "Hansenclever Bassani",
      "Arkady Epshteyn",
      "Hongbo Fan",
      "Isaac Jones",
      "Mahan Malihi",
      "Adrian Nauth",
      "Raj Sinha",
      "Sanjana Woonna",
      "Shiva Zamani",
      "Elli Kanal",
      "Mikhail Fomitchev",
      "Donny Cheung"
    ],
    "category": "cs.CL",
    "published_date": "2022-11-04 15:24:19+00:00",
    "updated_date": "2022-11-04 15:24:19+00:00",
    "arxiv_url": "http://arxiv.org/abs/2211.02519v1",
    "pdf_url": "https://arxiv.org/pdf/2211.02519v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2408.16495v1",
    "title": "On-device AI: Quantization-aware Training of Transformers in Time-Series",
    "abstract": "Artificial Intelligence (AI) models for time-series in pervasive computing keep getting larger and more complicated. The Transformer model is by far the most compelling of these AI models. However, it is difficult to obtain the desired performance when deploying such a massive model on a sensor device with limited resources. My research focuses on optimizing the Transformer model for time-series forecasting tasks. The optimized model will be deployed as hardware accelerators on embedded Field Programmable Gate Arrays (FPGAs). I will investigate the impact of applying Quantization-aware Training to the Transformer model to reduce its size and runtime memory footprint while maximizing the advantages of FPGAs.",
    "authors": [
      "Tianheng Ling",
      "Gregor Schiele"
    ],
    "category": "cs.LG",
    "published_date": "2024-08-29 12:49:22+00:00",
    "updated_date": "2024-08-29 12:49:22+00:00",
    "arxiv_url": "http://arxiv.org/abs/2408.16495v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16495v1.pdf",
    "doi": "http://dx.doi.org/10.1109/PerComWorkshops56833.2023.10150339",
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2504.14697v2",
    "title": "Quantitative Clustering in Mean-Field Transformer Models",
    "abstract": "The evolution of tokens through a deep transformer models can be modeled as an interacting particle system that has been shown to exhibit an asymptotic clustering behavior akin to the synchronization phenomenon in Kuramoto models. In this work, we investigate the long-time clustering of mean-field transformer models. More precisely, we establish exponential rates of contraction to a Dirac point mass for any suitably regular initialization under some assumptions on the parameters of transformer models, any suitably regular mean-field initialization synchronizes exponentially fast with some quantitative rates.",
    "authors": [
      "Shi Chen",
      "Zhengjiang Lin",
      "Yury Polyanskiy",
      "Philippe Rigollet"
    ],
    "category": "cs.LG",
    "published_date": "2025-04-20 18:21:34+00:00",
    "updated_date": "2025-04-30 13:35:39+00:00",
    "arxiv_url": "http://arxiv.org/abs/2504.14697v2",
    "pdf_url": "https://arxiv.org/pdf/2504.14697v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "0511508v1",
    "title": "Quantile regression in transformation models",
    "abstract": "Conditional quantiles provide a natural tool for reporting results from regression analyses based on semiparametric transformation models. We consider their estimation and construction of confidence sets in the presence of censoring.",
    "authors": [
      "Dorota M. Dabrowska"
    ],
    "category": "math.ST",
    "published_date": "2005-11-21 04:07:16+00:00",
    "updated_date": "2005-11-21 04:07:16+00:00",
    "arxiv_url": "http://arxiv.org/abs/math/0511508v1",
    "pdf_url": "https://arxiv.org/pdf/0511508v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1803.07416v1",
    "title": "Tensor2Tensor for Neural Machine Translation",
    "abstract": "Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.",
    "authors": [
      "Ashish Vaswani",
      "Samy Bengio",
      "Eugene Brevdo",
      "Francois Chollet",
      "Aidan N. Gomez",
      "Stephan Gouws",
      "Llion Jones",
      "\u0141ukasz Kaiser",
      "Nal Kalchbrenner",
      "Niki Parmar",
      "Ryan Sepassi",
      "Noam Shazeer",
      "Jakob Uszkoreit"
    ],
    "category": "cs.LG",
    "published_date": "2018-03-16 18:49:22+00:00",
    "updated_date": "2018-03-16 18:49:22+00:00",
    "arxiv_url": "http://arxiv.org/abs/1803.07416v1",
    "pdf_url": "https://arxiv.org/pdf/1803.07416v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1812.10758v1",
    "title": "Semiparametric Estimation for the Transformation Model with   Length-Biased Data and Covariate Measurement Error",
    "abstract": "Analysis of survival data with biased samples caused by left-truncation or length-biased sampling has received extensive interest. Many inference methods have been developed for various survival models. These methods, however, break down when survival data are typically error-contaminated. Although error-prone survival data commonly arise in practice, little work has been available in the literature for handling length-biased data with measurement error. In survival analysis, the transformation model is one of the frequently used models. However, methods of analyzing the transformation model with those complex features have not been fully explored. In this paper, we study this important problem and develop a valid inference method under the transformation model. We establish asymptotic results for the proposed estimators. The proposed method enjoys appealing features in that there is no need to specify the distribution of the covariates and the increasing function in the transformation model. Numerical studies are reported to assess the performance of the proposed method.",
    "authors": [
      "Li-Pang Chen"
    ],
    "category": "math.ST",
    "published_date": "2018-12-27 16:26:41+00:00",
    "updated_date": "2018-12-27 16:26:41+00:00",
    "arxiv_url": "http://arxiv.org/abs/1812.10758v1",
    "pdf_url": "https://arxiv.org/pdf/1812.10758v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2101.00387v2",
    "title": "What all do audio transformer models hear? Probing Acoustic   Representations for Language Delivery and its Structure",
    "abstract": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
    "authors": [
      "Jui Shah",
      "Yaman Kumar Singla",
      "Changyou Chen",
      "Rajiv Ratn Shah"
    ],
    "category": "cs.CL",
    "published_date": "2021-01-02 06:29:12+00:00",
    "updated_date": "2021-07-12 22:46:37+00:00",
    "arxiv_url": "http://arxiv.org/abs/2101.00387v2",
    "pdf_url": "https://arxiv.org/pdf/2101.00387v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2003.05683v1",
    "title": "Identification in a Fully Nonparametric Transformation Model with   Heteroscedasticity",
    "abstract": "The so far most general identification result in the context of nonparametric transformation models is proven. The result is constructive in the sense that it provides an explicit expression of the transformation function.",
    "authors": [
      "Nick Kloodt"
    ],
    "category": "math.ST",
    "published_date": "2020-03-12 09:50:35+00:00",
    "updated_date": "2020-03-12 09:50:35+00:00",
    "arxiv_url": "http://arxiv.org/abs/2003.05683v1",
    "pdf_url": "https://arxiv.org/pdf/2003.05683v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2311.11626v1",
    "title": "A novel transformer-based approach for soil temperature prediction",
    "abstract": "Soil temperature is one of the most significant parameters that plays a crucial role in glacier energy, dynamics of mass balance, processes of surface hydrological, coaction of glacier-atmosphere, nutrient cycling, ecological stability, the management of soil, water, and field crop. In this work, we introduce a novel approach using transformer models for the purpose of forecasting soil temperature prediction. To the best of our knowledge, the usage of transformer models in this work is the very first attempt to predict soil temperature. Experiments are carried out using six different FLUXNET stations by modeling them with five different transformer models, namely, Vanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To demonstrate the effectiveness of the proposed model, experiment results are compared with both deep learning approaches and literature studies. Experiment results show that the utilization of transformer models ensures a significant contribution to the literature, thence determining the new state-of-the-art.",
    "authors": [
      "Muhammet Mucahit Enes Yurtsever",
      "Ayhan Kucukmanisa",
      "Zeynep Hilal Kilimci"
    ],
    "category": "cs.LG",
    "published_date": "2023-11-20 09:20:26+00:00",
    "updated_date": "2023-11-20 09:20:26+00:00",
    "arxiv_url": "http://arxiv.org/abs/2311.11626v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11626v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2311.13755v1",
    "title": "Transformer-based Named Entity Recognition in Construction Supply Chain   Risk Management in Australia",
    "abstract": "The construction industry in Australia is characterized by its intricate supply chains and vulnerability to myriad risks. As such, effective supply chain risk management (SCRM) becomes imperative. This paper employs different transformer models, and train for Named Entity Recognition (NER) in the context of Australian construction SCRM. Utilizing NER, transformer models identify and classify specific risk-associated entities in news articles, offering a detailed insight into supply chain vulnerabilities. By analysing news articles through different transformer models, we can extract relevant entities and insights related to specific risk taxonomies local (milieu) to the Australian construction landscape. This research emphasises the potential of NLP-driven solutions, like transformer models, in revolutionising SCRM for construction in geo-media specific contexts.",
    "authors": [
      "Milad Baghalzadeh Shishehgarkhaneh",
      "Robert C. Moehler",
      "Yihai Fang",
      "Amer A. Hijazi",
      "Hamed Aboutorab"
    ],
    "category": "cs.CL",
    "published_date": "2023-11-23 01:06:08+00:00",
    "updated_date": "2023-11-23 01:06:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/2311.13755v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13755v1.pdf",
    "doi": "http://dx.doi.org/10.1109/ACCESS.2024.3377232",
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2402.06684v1",
    "title": "Ai4Fapar: How artificial intelligence can help to forecast the seasonal   earth observation signal",
    "abstract": "This paper investigated the potential of a multivariate Transformer model to forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1 month) periods at the regional level in Europe and North Africa. The input data covers the period from 2002 to 2022 and includes remote sensing and weather data for modelling FAPAR predictions. The model was evaluated using a leave one year out cross-validation and compared with the climatological benchmark. Results show that the transformer model outperforms the benchmark model for one month forecasting horizon, after which the climatological benchmark is better. The RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units for the first 2 months of predictions. Overall, the tested Transformer model is a valid method for FAPAR forecasting, especially when combined with weather data and used for short-term predictions.",
    "authors": [
      "Filip Sabo",
      "Martin Claverie",
      "Michele Meroni",
      "Arthur Hrast Essenfelder"
    ],
    "category": "physics.ao-ph",
    "published_date": "2024-02-08 11:00:51+00:00",
    "updated_date": "2024-02-08 11:00:51+00:00",
    "arxiv_url": "http://arxiv.org/abs/2402.06684v1",
    "pdf_url": "https://arxiv.org/pdf/2402.06684v1.pdf",
    "doi": "http://dx.doi.org/10.2760/46796",
    "journal_ref": "Proceedings of the 2023 conference on Big Data from Space, Soille,\n  P., Lumnitz, S. and Albani, S. editor(s), Publications Office of the European\n  Union, Luxembourg, 2023, JRC135493",
    "topic_category": "transformer"
  },
  {
    "id": "2406.07484v1",
    "title": "Towards Generalized Hydrological Forecasting using Transformer Models   for 120-Hour Streamflow Prediction",
    "abstract": "This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US. Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow. Our approach contrasts with traditional methods that typically rely on location-specific models. We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics. The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values. This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances. Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches.",
    "authors": [
      "Bekir Z. Demiray",
      "Ibrahim Demir"
    ],
    "category": "cs.LG",
    "published_date": "2024-06-11 17:26:14+00:00",
    "updated_date": "2024-06-11 17:26:14+00:00",
    "arxiv_url": "http://arxiv.org/abs/2406.07484v1",
    "pdf_url": "https://arxiv.org/pdf/2406.07484v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2110.04337v1",
    "title": "Adversarial Token Attacks on Vision Transformers",
    "abstract": "Vision transformers rely on a patch token based self attention mechanism, in contrast to convolutional networks. We investigate fundamental differences between these two families of models, by designing a block sparsity based adversarial token attack. We probe and analyze transformer as well as convolutional models with token attacks of varying patch sizes. We infer that transformer models are more sensitive to token attacks than convolutional models, with ResNets outperforming Transformer models by up to $\\sim30\\%$ in robust accuracy for single token attacks.",
    "authors": [
      "Ameya Joshi",
      "Gauri Jagatap",
      "Chinmay Hegde"
    ],
    "category": "cs.CV",
    "published_date": "2021-10-08 19:00:16+00:00",
    "updated_date": "2021-10-08 19:00:16+00:00",
    "arxiv_url": "http://arxiv.org/abs/2110.04337v1",
    "pdf_url": "https://arxiv.org/pdf/2110.04337v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2505.01575v2",
    "title": "Asset Pricing in Pre-trained Transformer",
    "abstract": "This paper proposes an innovative Transformer model, Single-directional representative from Transformer (SERT), for US large capital stock pricing. It also innovatively applies the pre-trained Transformer models under the stock pricing and factor investment context. They are compared with standard Transformer models and encoder-only Transformer models in three periods covering the entire COVID-19 pandemic to examine the model adaptivity and suitability during the extreme market fluctuations. Namely, pre-COVID-19 period (mild up-trend), COVID-19 period (sharp up-trend with deep down shock) and 1-year post-COVID-19 (high fluctuation sideways movement). The best proposed SERT model achieves the highest out-of-sample R2, 11.2% and 10.91% respectively, when extreme market fluctuation takes place followed by pre-trained Transformer models (10.38% and 9.15%). Their Trend-following-based strategy wise performance also proves their excellent capability for hedging downside risks during market shocks. The proposed SERT model achieves a Sortino ratio 47% higher than the buy-and-hold benchmark in the equal-weighted portfolio and 28% higher in the value-weighted portfolio when the pandemic period is attended. It proves that Transformer models have a great capability to capture patterns of temporal sparsity data in the asset pricing factor model, especially with considerable volatilities. We also find the softmax signal filter as the common configuration of Transformer models in alternative contexts, which only eliminates differences between models, but does not improve strategy-wise performance, while increasing attention heads improve the model performance insignificantly and applying the 'layer norm first' method do not boost the model performance in our case.",
    "authors": [
      "Shanyan Lai"
    ],
    "category": "q-fin.CP",
    "published_date": "2025-05-02 20:38:59+00:00",
    "updated_date": "2025-05-06 10:14:15+00:00",
    "arxiv_url": "http://arxiv.org/abs/2505.01575v2",
    "pdf_url": "https://arxiv.org/pdf/2505.01575v2.pdf",
    "doi": "http://dx.doi.org/10.5281/zenodo.15327831",
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2204.05454v1",
    "title": "Are Multimodal Transformers Robust to Missing Modality?",
    "abstract": "Multimodal data collected from the real world are often imperfect due to missing modalities. Therefore multimodal models that are robust against modal-incomplete data are highly preferred. Recently, Transformer models have shown great success in processing multimodal data. However, existing work has been limited to either architecture designs or pre-training strategies; whether Transformer models are naturally robust against missing-modal data has rarely been investigated. In this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of Transformers in the presence of modal-incomplete data. Unsurprising, we find Transformer models are sensitive to missing modalities while different modal fusion strategies will significantly affect the robustness. What surprised us is that the optimal fusion strategy is dataset dependent even for the same Transformer model; there does not exist a universal strategy that works in general cases. Based on these findings, we propose a principle method to improve the robustness of Transformer models by automatically searching for an optimal fusion strategy regarding input data. Experimental validations on three benchmarks support the superior performance of the proposed method.",
    "authors": [
      "Mengmeng Ma",
      "Jian Ren",
      "Long Zhao",
      "Davide Testuggine",
      "Xi Peng"
    ],
    "category": "cs.CV",
    "published_date": "2022-04-12 00:21:31+00:00",
    "updated_date": "2022-04-12 00:21:31+00:00",
    "arxiv_url": "http://arxiv.org/abs/2204.05454v1",
    "pdf_url": "https://arxiv.org/pdf/2204.05454v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2209.10966v2",
    "title": "Adaptation of domain-specific transformer models with text oversampling   for sentiment analysis of social media posts on Covid-19 vaccines",
    "abstract": "Covid-19 has spread across the world and several vaccines have been developed to counter its surge. To identify the correct sentiments associated with the vaccines from social media posts, we fine-tune various state-of-the-art pre-trained transformer models on tweets associated with Covid-19 vaccines. Specifically, we use the recently introduced state-of-the-art pre-trained transformer models RoBERTa, XLNet and BERT, and the domain-specific transformer models CT-BERT and BERTweet that are pre-trained on Covid-19 tweets. We further explore the option of text augmentation by oversampling using Language Model based Oversampling Technique (LMOTE) to improve the accuracies of these models, specifically, for small sample datasets where there is an imbalanced class distribution among the positive, negative and neutral sentiment classes. Our results summarize our findings on the suitability of text oversampling for imbalanced small sample datasets that are used to fine-tune state-of-the-art pre-trained transformer models, and the utility of domain-specific transformer models for the classification task.",
    "authors": [
      "Anmol Bansal",
      "Arjun Choudhry",
      "Anubhav Sharma",
      "Seba Susan"
    ],
    "category": "cs.CL",
    "published_date": "2022-09-22 12:36:40+00:00",
    "updated_date": "2023-01-13 07:45:21+00:00",
    "arxiv_url": "http://arxiv.org/abs/2209.10966v2",
    "pdf_url": "https://arxiv.org/pdf/2209.10966v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2302.09108v1",
    "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
    "abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
    "authors": [
      "Shashank Nag",
      "Gourav Datta",
      "Souvik Kundu",
      "Nitin Chandrachoodan",
      "Peter A. Beerel"
    ],
    "category": "cs.AR",
    "published_date": "2023-02-17 19:35:36+00:00",
    "updated_date": "2023-02-17 19:35:36+00:00",
    "arxiv_url": "http://arxiv.org/abs/2302.09108v1",
    "pdf_url": "https://arxiv.org/pdf/2302.09108v1.pdf",
    "doi": "http://dx.doi.org/10.1109/ISCAS46773.2023.10181988",
    "journal_ref": "2023 IEEE International Symposium on Circuits and Systems (ISCAS),\n  Monterey, CA, USA, 2023, pp. 1-5",
    "topic_category": "transformer"
  },
  {
    "id": "2405.02353v1",
    "title": "Early Transformers: A study on Efficient Training of Transformer Models   through Early-Bird Lottery Tickets",
    "abstract": "The training of Transformer models has revolutionized natural language processing and computer vision, but it remains a resource-intensive and time-consuming process. This paper investigates the applicability of the early-bird ticket hypothesis to optimize the training efficiency of Transformer models. We propose a methodology that combines iterative pruning, masked distance calculation, and selective retraining to identify early-bird tickets in various Transformer architectures, including ViT, Swin-T, GPT-2, and RoBERTa. Our experimental results demonstrate that early-bird tickets can be consistently found within the first few epochs of training or fine-tuning, enabling significant resource optimization without compromising performance. The pruned models obtained from early-bird tickets achieve comparable or even superior accuracy to their unpruned counterparts while substantially reducing memory usage. Furthermore, our comparative analysis highlights the generalizability of the early-bird ticket phenomenon across different Transformer models and tasks. This research contributes to the development of efficient training strategies for Transformer models, making them more accessible and resource-friendly. By leveraging early-bird tickets, practitioners can accelerate the progress of natural language processing and computer vision applications while reducing the computational burden associated with training Transformer models.",
    "authors": [
      "Shravan Cheekati"
    ],
    "category": "cs.CL",
    "published_date": "2024-05-02 23:03:45+00:00",
    "updated_date": "2024-05-02 23:03:45+00:00",
    "arxiv_url": "http://arxiv.org/abs/2405.02353v1",
    "pdf_url": "https://arxiv.org/pdf/2405.02353v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2507.11729v1",
    "title": "Globalization for Scalable Short-term Load Forecasting",
    "abstract": "Forecasting load in power transmission networks is essential across various hierarchical levels, from the system level down to individual points of delivery (PoD). While intuitive and locally accurate, traditional local forecasting models (LFMs) face significant limitations, particularly in handling generalizability, overfitting, data drift, and the cold start problem. These methods also struggle with scalability, becoming computationally expensive and less efficient as the network's size and data volume grow. In contrast, global forecasting models (GFMs) offer a new approach to enhance prediction generalizability, scalability, accuracy, and robustness through globalization and cross-learning. This paper investigates global load forecasting in the presence of data drifts, highlighting the impact of different modeling techniques and data heterogeneity. We explore feature-transforming and target-transforming models, demonstrating how globalization, data heterogeneity, and data drift affect each differently. In addition, we examine the role of globalization in peak load forecasting and its potential for hierarchical forecasting. To address data heterogeneity and the balance between globality and locality, we propose separate time series clustering (TSC) methods, introducing model-based TSC for feature-transforming models and new weighted instance-based TSC for target-transforming models. Through extensive experiments on a real-world dataset of Alberta's electricity load, we demonstrate that global target-transforming models consistently outperform their local counterparts, especially when enriched with global features and clustering techniques. In contrast, global feature-transforming models face challenges in balancing local and global dynamics, often requiring TSC to manage data heterogeneity effectively.",
    "authors": [
      "Amirhossein Ahmadi",
      "Hamidreza Zareipour",
      "Henry Leung"
    ],
    "category": "cs.LG",
    "published_date": "2025-07-15 20:58:14+00:00",
    "updated_date": "2025-07-15 20:58:14+00:00",
    "arxiv_url": "http://arxiv.org/abs/2507.11729v1",
    "pdf_url": "https://arxiv.org/pdf/2507.11729v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2104.14528v7",
    "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for   Gastric Histopathological Image Detection",
    "abstract": "In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.",
    "authors": [
      "Haoyuan Chen",
      "Chen Li",
      "Ge Wang",
      "Xiaoyan Li",
      "Md Rahaman",
      "Hongzan Sun",
      "Weiming Hu",
      "Yixin Li",
      "Wanli Liu",
      "Changhao Sun",
      "Shiliang Ai",
      "Marcin Grzegorzek"
    ],
    "category": "cs.CV",
    "published_date": "2021-04-29 17:46:00+00:00",
    "updated_date": "2022-06-08 11:26:17+00:00",
    "arxiv_url": "http://arxiv.org/abs/2104.14528v7",
    "pdf_url": "https://arxiv.org/pdf/2104.14528v7.pdf",
    "doi": "http://dx.doi.org/10.1016/j.patcog.2022.108827",
    "journal_ref": "Pattern Recognition Volume 130, October 2022, 108827",
    "topic_category": "transformer"
  },
  {
    "id": "2105.14077v1",
    "title": "On the Bias Against Inductive Biases",
    "abstract": "Borrowing from the transformer models that revolutionized the field of natural language processing, self-supervised feature learning for visual tasks has also seen state-of-the-art success using these extremely deep, isotropic networks. However, the typical AI researcher does not have the resources to evaluate, let alone train, a model with several billion parameters and quadratic self-attention activations. To facilitate further research, it is necessary to understand the features of these huge transformer models that can be adequately studied by the typical researcher. One interesting characteristic of these transformer models is that they remove most of the inductive biases present in classical convolutional networks. In this work, we analyze the effect of these and more inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning and show that their removal is not always ideal.",
    "authors": [
      "George Cazenavette",
      "Simon Lucey"
    ],
    "category": "cs.CV",
    "published_date": "2021-05-28 19:41:48+00:00",
    "updated_date": "2021-05-28 19:41:48+00:00",
    "arxiv_url": "http://arxiv.org/abs/2105.14077v1",
    "pdf_url": "https://arxiv.org/pdf/2105.14077v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1912.10077v2",
    "title": "Are Transformers universal approximators of sequence-to-sequence   functions?",
    "abstract": "Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to self-attention layers and empirically evaluate them.",
    "authors": [
      "Chulhee Yun",
      "Srinadh Bhojanapalli",
      "Ankit Singh Rawat",
      "Sashank J. Reddi",
      "Sanjiv Kumar"
    ],
    "category": "cs.LG",
    "published_date": "2019-12-20 19:49:32+00:00",
    "updated_date": "2020-02-25 03:12:57+00:00",
    "arxiv_url": "http://arxiv.org/abs/1912.10077v2",
    "pdf_url": "https://arxiv.org/pdf/1912.10077v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "2012.02144v1",
    "title": "Do We Really Need That Many Parameters In Transformer For Extractive   Summarization? Discourse Can Help !",
    "abstract": "The multi-head self-attention of popular transformer models is widely used within Natural Language Processing (NLP), including for the task of extractive summarization. With the goal of analyzing and pruning the parameter-heavy self-attention mechanism, there are multiple approaches proposing more parameter-light self-attention alternatives. In this paper, we present a novel parameter-lean self-attention mechanism using discourse priors. Our new tree self-attention is based on document-level discourse information, extending the recently proposed \"Synthesizer\" framework with another lightweight alternative. We show empirical results that our tree self-attention approach achieves competitive ROUGE-scores on the task of extractive summarization. When compared to the original single-head transformer model, the tree attention approach reaches similar performance on both, EDU and sentence level, despite the significant reduction of parameters in the attention component. We further significantly outperform the 8-head transformer model on sentence level when applying a more balanced hyper-parameter setting, requiring an order of magnitude less parameters.",
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ],
    "category": "cs.CL",
    "published_date": "2020-12-03 18:23:21+00:00",
    "updated_date": "2020-12-03 18:23:21+00:00",
    "arxiv_url": "http://arxiv.org/abs/2012.02144v1",
    "pdf_url": "https://arxiv.org/pdf/2012.02144v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "transformer"
  },
  {
    "id": "1902.03367v1",
    "title": "Unnormalized Optimal Transport",
    "abstract": "We propose an extension of the computational fluid mechanics approach to the Monge-Kantorovich mass transfer problem, which was developed by Benamou-Brenier. Our extension allows optimal transfer of unnormalized and unequal masses. We obtain a one-parameter family of simple modifications of the formulation in [4]. This leads us to a new Monge-Ampere type equation and a new Kantorovich duality formula. These can be solved efficiently by, for example, the Chambolle-Pock primal-dual algorithm. This solution to the extended mass transfer problem gives us a simple metric for computing the distance between two unnormalized densities. The L1 version of this metric was shown in [23] (which is a precursor of our work here) to have desirable properties.",
    "authors": [
      "Wilfrid Gangbo",
      "Wuchen Li",
      "Stanley Osher",
      "Michael Puthawala"
    ],
    "category": "math.OC",
    "published_date": "2019-02-09 04:05:10+00:00",
    "updated_date": "2019-02-09 04:05:10+00:00",
    "arxiv_url": "http://arxiv.org/abs/1902.03367v1",
    "pdf_url": "https://arxiv.org/pdf/1902.03367v1.pdf",
    "doi": "http://dx.doi.org/10.1016/j.jcp.2019.108940",
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1807.00393v2",
    "title": "Adaptive Optimal Transport",
    "abstract": "An adaptive, adversarial methodology is developed for the optimal transport problem between two distributions $\\mu$ and $\\nu$, known only through a finite set of independent samples $(x_i)_{i=1..N}$ and $(y_j)_{j=1..M}$. The methodology automatically creates features that adapt to the data, thus avoiding reliance on a priori knowledge of data distribution. Specifically, instead of a discrete point-bypoint assignment, the new procedure seeks an optimal map $T(x)$ defined for all $x$, minimizing the Kullback-Leibler divergence between $(T(xi))$ and the target $(y_j)$. The relative entropy is given a sample-based, variational characterization, thereby creating an adversarial setting: as one player seeks to push forward one distribution to the other, the second player develops features that focus on those areas where the two distributions fail to match. The procedure solves local problems matching consecutive, intermediate distributions between $\\mu$ and $\\nu$. As a result, maps of arbitrary complexity can be built by composing the simple maps used for each local problem. Displaced interpolation is used to guarantee global from local optimality. The procedure is illustrated through synthetic examples in one and two dimensions.",
    "authors": [
      "Montacer Essid",
      "Debra Laefer",
      "Esteban G. Tabak"
    ],
    "category": "math.OC",
    "published_date": "2018-07-01 20:48:02+00:00",
    "updated_date": "2019-02-18 22:12:18+00:00",
    "arxiv_url": "http://arxiv.org/abs/1807.00393v2",
    "pdf_url": "https://arxiv.org/pdf/1807.00393v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2206.13410v1",
    "title": "Supervised Optimal Transport",
    "abstract": "Optimal Transport, a theory for optimal allocation of resources, is widely used in various fields such as astrophysics, machine learning, and imaging science. However, many applications impose elementwise constraints on the transport plan which traditional optimal transport cannot enforce. Here we introduce Supervised Optimal Transport (sOT) that formulates a constrained optimal transport problem where couplings between certain elements are prohibited according to specific applications. sOT is proved to be equivalent to an $l^1$ penalized optimization problem, from which efficient algorithms are designed to solve its entropy regularized formulation. We demonstrate the capability of sOT by comparing it to other variants and extensions of traditional OT in color transfer problem. We also study the barycenter problem in sOT formulation, where we discover and prove a unique reverse and portion selection (control) mechanism. Supervised optimal transport is broadly applicable to applications in which constrained transport plan is involved and the original unit should be preserved by avoiding normalization.",
    "authors": [
      "Zixuan Cang",
      "Qing Nie",
      "Yanxiang Zhao"
    ],
    "category": "math.OC",
    "published_date": "2022-06-27 16:00:59+00:00",
    "updated_date": "2022-06-27 16:00:59+00:00",
    "arxiv_url": "http://arxiv.org/abs/2206.13410v1",
    "pdf_url": "https://arxiv.org/pdf/2206.13410v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2304.00392v1",
    "title": "Optimal Transport Particle Filters",
    "abstract": "This paper is concerned with the theoretical and computational development of a new class of nonlinear filtering algorithms called the optimal transport particle filters (OTPF). The algorithm is based on a recently introduced variational formulation of the Bayes' rule, which aims to find the Brenier optimal transport map between the prior and the posterior distributions as the solution to a stochastic optimization problem. On the theoretical side, the existing methods for the error analysis of particle filters and stability results for optimal transport map estimation are combined to obtain uniform error bounds for the filter's performance in terms of the optimization gap in solving the variational problem. The error analysis reveals a bias-variance trade-off that can ultimately be used to understand if/when the curse of dimensionality can be avoided in these filters. On the computational side, the proposed algorithm is evaluated on a nonlinear filtering example in comparison with the ensemble Kalman filter (EnKF) and the sequential importance resampling (SIR) particle filter.",
    "authors": [
      "Mohammad Al-Jarrah",
      "Bamdad Hosseini",
      "Amirhossein Taghvaei"
    ],
    "category": "math.OC",
    "published_date": "2023-04-01 21:17:36+00:00",
    "updated_date": "2023-04-01 21:17:36+00:00",
    "arxiv_url": "http://arxiv.org/abs/2304.00392v1",
    "pdf_url": "https://arxiv.org/pdf/2304.00392v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2502.10607v1",
    "title": "Time Parameterized Optimal Transport",
    "abstract": "Optimal transport has gained significant attention in recent years due to its effectiveness in deep learning and computer vision. Its descendant metric, the Wasserstein distance, has been particularly successful in measuring distribution dissimilarities. While extensive research has focused on optimal transport and its regularized variants (such as entropy, sparsity, and capacity constraints) the role of time has been largely overlooked. However, time is a critical factor in real world transport problems.   In this work, we introduce a time parameterized formulation of the optimal transport problem, incorporating a time variable t to represent sequential steps and enforcing specific constraints at each step. We propose a systematic method to solve a special subproblem and develop a heuristic search algorithm that achieves nearly optimal solutions while significantly reducing computational time.",
    "authors": [
      "Kaiwen Shi"
    ],
    "category": "math.OC",
    "published_date": "2025-02-14 23:48:13+00:00",
    "updated_date": "2025-02-14 23:48:13+00:00",
    "arxiv_url": "http://arxiv.org/abs/2502.10607v1",
    "pdf_url": "https://arxiv.org/pdf/2502.10607v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2508.01243v1",
    "title": "Sliced Optimal Transport Plans",
    "abstract": "Since the introduction of the Sliced Wasserstein distance in the literature, its simplicity and efficiency have made it one of the most interesting surrogate for the Wasserstein distance in image processing and machine learning. However, its inability to produce transport plans limits its practical use to applications where only a distance is necessary. Several heuristics have been proposed in the recent years to address this limitation when the probability measures are discrete. In this paper, we propose to study these different propositions by redefining and analysing them rigorously for generic probability measures. Leveraging the $\\nu$-based Wasserstein distance and generalised geodesics, we introduce and study the Pivot Sliced Discrepancy, inspired by a recent work by Mahey et al.. We demonstrate its semi-metric properties and its relation to a constrained Kantorovich formulation. In the same way, we generalise and study the recent Expected Sliced plans introduced by Liu et al. for completely generic measures. Our theoretical contributions are supported by numerical experiments on synthetic and real datasets, including colour transfer and shape registration, evaluating the practical relevance of these different solutions.",
    "authors": [
      "Eloi Tanguy",
      "Laetitia Chapel",
      "Julie Delon"
    ],
    "category": "math.OC",
    "published_date": "2025-08-02 07:38:28+00:00",
    "updated_date": "2025-08-02 07:38:28+00:00",
    "arxiv_url": "http://arxiv.org/abs/2508.01243v1",
    "pdf_url": "https://arxiv.org/pdf/2508.01243v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "0612142v1",
    "title": "Optimal transportation for the determinant",
    "abstract": "Among $\\R^3$-valued triples of random vectors $(X,Y,Z)$ having fixed marginal probability laws, what is the best way to jointly draw $(X,Y,Z)$ in such a way that the simplex generated by $(X,Y,Z)$ has maximal average volume? Motivated by this simple question, we study optimal transportation problems with several marginals when the objective function is the determinant or its absolute value.",
    "authors": [
      "Guillaume Carlier",
      "Bruno Nazaret"
    ],
    "category": "math.OC",
    "published_date": "2006-12-06 06:07:08+00:00",
    "updated_date": "2006-12-06 06:07:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/math/0612142v1",
    "pdf_url": "https://arxiv.org/pdf/0612142v1.pdf",
    "doi": "http://dx.doi.org/10.1051/cocv:2008006",
    "journal_ref": "ESAIM - Control Optimisation and Calculus of Variations 14, 4\n  (2008) p678-698",
    "topic_category": "optimal_transport"
  },
  {
    "id": "1201.6404v3",
    "title": "Optimal Transportation with Capacity Constraints",
    "abstract": "The classical problem of optimal transportation can be formulated as a linear optimization problem on a convex domain: among all joint measures with fixed marginals find the optimal one, where optimality is measured against a cost function. Here we consider a natural but largely unexplored variant of this problem by imposing a pointwise constraint on the joint (absolutely continuous) measures: among all joint densities with fixed marginals and which are dominated by a given density, find the optimal one. For this variant, we show local non-degeneracy of the cost function implies every minimizer is extremal in the convex set of competitors, hence unique. An appendix develops rudiments of a duality theory for this problem, which allows us to compute several suggestive examples.",
    "authors": [
      "Jonathan Korman",
      "Robert J. McCann"
    ],
    "category": "math.OC",
    "published_date": "2012-01-30 23:27:55+00:00",
    "updated_date": "2012-11-28 04:39:22+00:00",
    "arxiv_url": "http://arxiv.org/abs/1201.6404v3",
    "pdf_url": "https://arxiv.org/pdf/1201.6404v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1703.08442v3",
    "title": "Equilibrium selection via Optimal transport",
    "abstract": "We propose a new dynamics for equilibrium selection of finite player discrete strategy games. The dynamics is motivated by optimal transportation, and models individual players' myopicity, greedy and uncertainty when making decisions. The stationary measure of the dynamics provides each pure Nash equilibrium a probability by which it is ranked. For potential games, its dynamical properties are characterized by entropy and Fisher information.",
    "authors": [
      "Shui-Nee Chow",
      "Wuchen Li",
      "Jun Lu",
      "Haomin Zhou"
    ],
    "category": "math.OC",
    "published_date": "2017-03-23 06:28:24+00:00",
    "updated_date": "2017-07-24 18:32:15+00:00",
    "arxiv_url": "http://arxiv.org/abs/1703.08442v3",
    "pdf_url": "https://arxiv.org/pdf/1703.08442v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2002.01189v2",
    "title": "From Optimal Transport to Discrepancy",
    "abstract": "A common way to quantify the ,,distance'' between measures is via their discrepancy, also known as maximum mean discrepancy (MMD). Discrepancies are related to Sinkhorn divergences $S_\\varepsilon$ with appropriate cost functions as $\\varepsilon \\to \\infty$. In the opposite direction, if $\\varepsilon \\to 0$, Sinkhorn divergences approach another important distance between measures, namely the Wasserstein distance or more generally optimal transport ,,distance''. In this chapter, we investigate the limiting process for arbitrary measures on compact sets and Lipschitz continuous cost functions. In particular, we are interested in the behavior of the corresponding optimal potentials $\\hat \\varphi_\\varepsilon$, $\\hat \\psi_\\varepsilon$ and $\\hat \\varphi_K$ appearing in the dual formulation of the Sinkhorn divergences and discrepancies, respectively. While part of the results are known, we provide rigorous proofs for some relations which we have not found in this generality in the literature. Finally, we demonstrate the limiting process by numerical examples and show the behavior of the distances when used for the approximation of measures by point measures in a process called dithering.",
    "authors": [
      "Sebastian Neumayer",
      "Gabriele Steidl"
    ],
    "category": "math.OC",
    "published_date": "2020-02-04 09:38:07+00:00",
    "updated_date": "2020-08-24 13:07:30+00:00",
    "arxiv_url": "http://arxiv.org/abs/2002.01189v2",
    "pdf_url": "https://arxiv.org/pdf/2002.01189v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2103.03628v1",
    "title": "Deep Semi-Martingale Optimal Transport",
    "abstract": "We propose two deep neural network-based methods for solving semi-martingale optimal transport problems. The first method is based on a relaxation/penalization of the terminal constraint, and is solved using deep neural networks. The second method is based on the dual formulation of the problem, which we express as a saddle point problem, and is solved using adversarial networks. Both methods are mesh-free and therefore mitigate the curse of dimensionality. We test the performance and accuracy of our methods on several examples up to dimension 10. We also apply the first algorithm to a portfolio optimization problem where the goal is, given an initial wealth distribution, to find an investment strategy leading to a prescribed terminal wealth distribution.",
    "authors": [
      "Ivan Guo",
      "Nicolas Langren\u00e9",
      "Gr\u00e9goire Loeper",
      "Wei Ning"
    ],
    "category": "math.OC",
    "published_date": "2021-03-05 12:22:18+00:00",
    "updated_date": "2021-03-05 12:22:18+00:00",
    "arxiv_url": "http://arxiv.org/abs/2103.03628v1",
    "pdf_url": "https://arxiv.org/pdf/2103.03628v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2103.10854v3",
    "title": "Unbalanced Multi-Marginal Optimal Transport",
    "abstract": "Entropy regularized optimal transport and its multi-marginal generalization have attracted increasing attention in various applications, in particular due to efficient Sinkhorn-like algorithms for computing optimal transport plans. However, it is often desirable that the marginals of the optimal transport plan do not match the given measures exactly, which led to the introduction of the so-called unbalanced optimal transport. Since unbalanced methods were not examined for the multi-marginal setting so far, we address this topic in the present paper. More precisely, we introduce the unbalanced multi-marginal optimal transport problem and its dual, and show that a unique optimal transport plan exists under mild assumptions. Further, we generalize the Sinkhorn algorithm for regularized unbalanced optimal transport to the multi-marginal setting and prove its convergence. If the cost function decouples according to a tree, the iterates can be computed efficiently. At the end, we discuss three applications of our framework, namely two barycenter problems and a transfer operator approach, where we establish a relation between the barycenter problem and the multi-marginal optimal transport with an appropriate tree-structured cost function.",
    "authors": [
      "Florian Beier",
      "Johannes von Lindheim",
      "Sebastian Neumayer",
      "Gabriele Steidl"
    ],
    "category": "math.OC",
    "published_date": "2021-03-19 15:23:31+00:00",
    "updated_date": "2022-09-29 13:17:53+00:00",
    "arxiv_url": "http://arxiv.org/abs/2103.10854v3",
    "pdf_url": "https://arxiv.org/pdf/2103.10854v3.pdf",
    "doi": "http://dx.doi.org/10.1007/s10851-022-01126-7",
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1910.11422v1",
    "title": "Data Driven Conditional Optimal Transport",
    "abstract": "A data driven procedure is developed to compute the optimal map between two conditional probabilities $\\rho(x|z_{1},...,z_{L})$ and $\\mu(y|z_{1},...,z_{L})$ depending on a set of covariates $z_{i}$. The procedure is tested on synthetic data from the ACIC Data Analysis Challenge 2017 and it is applied to non uniform lightness transfer between images. Exactly solvable examples and simulations are performed to highlight the differences with ordinary optimal transport.",
    "authors": [
      "Esteban G. Tabak",
      "Giulio Trigila",
      "Wenjun Zhao"
    ],
    "category": "math.OC",
    "published_date": "2019-10-24 20:56:34+00:00",
    "updated_date": "2019-10-24 20:56:34+00:00",
    "arxiv_url": "http://arxiv.org/abs/1910.11422v1",
    "pdf_url": "https://arxiv.org/pdf/1910.11422v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2401.06266v1",
    "title": "Supervised Gromov-Wasserstein Optimal Transport",
    "abstract": "We introduce the supervised Gromov-Wasserstein (sGW) optimal transport, an extension of Gromov-Wasserstein by incorporating potential infinity patterns in the cost tensor. sGW enables the enforcement of application-induced constraints such as the preservation of pairwise distances by implementing the constraints as an infinity pattern. A numerical solver is proposed for the sGW problem and the effectiveness is demonstrated in various numerical experiments. The high-order constraints in sGW are transferred to constraints on the coupling matrix by solving a minimal vertex cover problem. The transformed problem is solved by the Mirror-C descent iteration coupled with the supervised optimal transport solver. In the numerical experiments, we first validate the proposed framework by applying it to matching synthetic datasets and investigating the impact of the model parameters. Additionally, we successfully apply sGW to real single-cell RNA sequencing data. Through comparisons with other Gromov-Wasserstein variants on real data, we demonstrate that sGW offers the novel utility of controlling distance preservation, leading to the automatic estimation of overlapping portions of datasets, which brings improved stability and flexibility in data-driven applications.",
    "authors": [
      "Zixuan Cang",
      "Yaqi Wu",
      "Yanxiang Zhao"
    ],
    "category": "math.OC",
    "published_date": "2024-01-11 21:27:00+00:00",
    "updated_date": "2024-01-11 21:27:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/2401.06266v1",
    "pdf_url": "https://arxiv.org/pdf/2401.06266v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2406.15503v1",
    "title": "Data representation with optimal transport",
    "abstract": "Optimal transport has been used to define bijective nonlinear transforms and different transport-related metrics for discriminating data and signals. Here we briefly describe the advances in this topic with the main applications and properties in each case.",
    "authors": [
      "Roc\u00edo D\u00edaz Mart\u00edn",
      "Ivan V. Medri",
      "Gustavo Kunde Rohde"
    ],
    "category": "math.OC",
    "published_date": "2024-06-19 15:36:41+00:00",
    "updated_date": "2024-06-19 15:36:41+00:00",
    "arxiv_url": "http://arxiv.org/abs/2406.15503v1",
    "pdf_url": "https://arxiv.org/pdf/2406.15503v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2407.13445v2",
    "title": "Constrained Approximate Optimal Transport Maps",
    "abstract": "We investigate finding a map $g$ within a function class $G$ that minimises an Optimal Transport (OT) cost between a target measure $\\nu$ and the image by $g$ of a source measure $\\mu$. This is relevant when an OT map from $\\mu$ to $\\nu$ does not exist or does not satisfy the desired constraints of $G$. We address existence and uniqueness for generic subclasses of $L$-Lipschitz functions, including gradients of (strongly) convex functions and typical Neural Networks. We explore a variant that approaches a transport plan, showing equivalence to a map problem in some cases. For the squared Euclidean cost, we propose alternating minimisation over a transport plan $\\pi$ and map $g$, with the optimisation over $g$ being the $L^2$ projection on $G$ of the barycentric mapping $\\overline{\\pi}$. In dimension one, this global problem equates the $L^2$ projection of $\\overline{\\pi^*}$ onto $G$ for an OT plan $\\pi^*$ between $\\mu$ and $\\nu$, but this does not extend to higher dimensions. We introduce a simple kernel method to find $g$ within a Reproducing Kernel Hilbert Space in the discrete case. We present numerical methods for $L$-Lipschitz gradients of $\\ell$-strongly convex potentials, and study the convergence of Stochastic Gradient Descent methods for Neural Networks. We finish with an illustration on colour transfer, applying learned maps on new images, and showcasing outlier robustness.",
    "authors": [
      "Eloi Tanguy",
      "Agn\u00e8s Desolneux",
      "Julie Delon"
    ],
    "category": "math.OC",
    "published_date": "2024-07-18 12:14:18+00:00",
    "updated_date": "2025-03-12 13:29:38+00:00",
    "arxiv_url": "http://arxiv.org/abs/2407.13445v2",
    "pdf_url": "https://arxiv.org/pdf/2407.13445v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2408.09361v3",
    "title": "Entropic Semi-Martingale Optimal Transport",
    "abstract": "Entropic Optimal Transport (EOT), also referred to as the Schr\\\"odinger problem, seeks to find a random processes with prescribed initial/final marginals and with minimal relative entropy with respect to a reference measure. The relative entropy forces the two measures to share the same support and only the drift of the controlled process can be adjusted, the diffusion being imposed by the reference measure. Therefore, at first sight, Semi-Martingale Optimal Transport (SMOT) problems (see [1]) seem out of the scope of applications of Entropic regularization techniques, which are otherwise very attractive from a computational point of view. However, when the process is observed only at discrete times, and become therefore a Markov chain, its relative entropy can remain finite even with variable diffusion coefficients, and discrete semi-martingales can be obtained as solutions of (multi-marginal) EOT problems.Given a (smooth) semi-martingale, the limit of the relative entropy of its time discretizations, scaled by the time step converges to the so-called ``specific relative entropy'', a convex functional of its variance process, similar to those used in SMOT.In this paper we use this observation to build an entropic time discretization of continuous SMOT problems. This allows to compute discrete approximations of solutions to continuous SMOT problems by a multi-marginal Sinkhorn algorithm, without the need of solving the non-linear Hamilton-Jacobi-Bellman pde's associated to the dual problem, as done for example in [1, 2]. We prove a convergence result of the time discrete entropic problem to the continuous time problem, we propose an implementation and provide numerical experiments supporting the theoretical convergence.",
    "authors": [
      "Jean-David Benamou",
      "Guillaume Chazareix",
      "Marc Hoffmann",
      "Gr\u00e9goire Loeper",
      "Fran\u00e7ois-Xavier Vialard"
    ],
    "category": "math.OC",
    "published_date": "2024-08-18 05:00:01+00:00",
    "updated_date": "2024-12-16 15:41:21+00:00",
    "arxiv_url": "http://arxiv.org/abs/2408.09361v3",
    "pdf_url": "https://arxiv.org/pdf/2408.09361v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1905.03950v1",
    "title": "Inverse optimal transport",
    "abstract": "Discrete optimal transportation problems arise in various contexts in engineering, the sciences and the social sciences. Often the underlying cost criterion is unknown, or only partly known, and the observed optimal solutions are corrupted by noise. In this paper we propose a systematic approach to infer unknown costs from noisy observations of optimal transportation plans. The algorithm requires only the ability to solve the forward optimal transport problem, which is a linear program, and to generate random numbers. It has a Bayesian interpretation, and may also be viewed as a form of stochastic optimization.   We illustrate the developed methodologies using the example of international migration flows. Reported migration flow data captures (noisily) the number of individuals moving from one country to another in a given period of time. It can be interpreted as a noisy observation of an optimal transportation map, with costs related to the geographical position of countries. We use a graph-based formulation of the problem, with countries at the nodes of graphs and non-zero weighted adjacencies only on edges between countries which share a border. We use the proposed algorithm to estimate the weights, which represent cost of transition, and to quantify uncertainty in these weights.",
    "authors": [
      "Andrew M. Stuart",
      "Marie-Therese Wolfram"
    ],
    "category": "math.OC",
    "published_date": "2019-05-10 05:51:08+00:00",
    "updated_date": "2019-05-10 05:51:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/1905.03950v1",
    "pdf_url": "https://arxiv.org/pdf/1905.03950v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2105.06922v2",
    "title": "Quantum Optimal Transport",
    "abstract": "We analyze a quantum version of the Monge--Kantorovich optimal transport problem. The quantum transport cost related to a Hermitian cost matrix $C$ is minimized over the set of all bipartite coupling states $\\rho^{AB}$ with fixed reduced density matrices $\\rho^A$ and $\\rho^B$ of size $m$ and $n$. The minimum quantum optimal transport cost $\\rT^Q_{C}(\\rho^A,\\rho^B)$ can be efficiently computed using semidefinite programming. In the case $m=n$ the cost $\\rT^Q_{C}$ gives a semidistance if and only if $C$ is positive semidefinite and vanishes exactly on the subspace of symmetric matrices. Furthermore, if $C$ satisfies the above conditions, then $\\sqrt{\\rT^Q_{C}}$ induces a quantum analogue of the Wasserstein-2 distance. Taking the quantum cost matrix $C^Q$ to be the projector on the antisymmetric subspace, we provide a semi-analytic expression for $\\rT^Q_{C^Q}$ for any pair of single-qubit states and show that its square root yields a transport distance on the Bloch ball. Numerical simulations suggest that this property holds also in higher dimensions. Assuming that the cost matrix suffers decoherence and that the density matrices become diagonal, we study the quantum-to-classical transition of the Earth mover's distance, propose a continuous family of interpolating distances, and demonstrate that the quantum transport is cheaper than the classical one. Furthermore, we introduce a related quantity -- the SWAP-fidelity -- and compare its properties with the standard Uhlmann--Jozsa fidelity. We also discuss the quantum optimal transport for general $d$-partite systems.",
    "authors": [
      "Sam Cole",
      "Micha\u0142 Eckstein",
      "Shmuel Friedland",
      "Karol \u017byczkowski"
    ],
    "category": "quant-ph",
    "published_date": "2021-05-14 16:11:27+00:00",
    "updated_date": "2022-07-12 16:53:36+00:00",
    "arxiv_url": "http://arxiv.org/abs/2105.06922v2",
    "pdf_url": "https://arxiv.org/pdf/2105.06922v2.pdf",
    "doi": "http://dx.doi.org/10.1007/s11040-023-09456-7",
    "journal_ref": "Math Phys Anal Geom 26, 14 (2023)",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2306.07176v2",
    "title": "Slicing Unbalanced Optimal Transport",
    "abstract": "Optimal transport (OT) is a powerful framework to compare probability measures, a fundamental task in many statistical and machine learning problems. Substantial advances have been made in designing OT variants which are either computationally and statistically more efficient or robust. Among them, sliced OT distances have been extensively used to mitigate optimal transport's cubic algorithmic complexity and curse of dimensionality. In parallel, unbalanced OT was designed to allow comparisons of more general positive measures, while being more robust to outliers. In this paper, we bridge the gap between those two concepts and develop a general framework for efficiently comparing positive measures. We notably formulate two different versions of sliced unbalanced OT, and study the associated topology and statistical properties. We then develop a GPU-friendly Frank-Wolfe like algorithm to compute the corresponding loss functions, and show that the resulting methodology is modular as it encompasses and extends prior related work. We finally conduct an empirical analysis of our loss functions and methodology on both synthetic and real datasets, to illustrate their computational efficiency, relevance and applicability to real-world scenarios including geophysical data.",
    "authors": [
      "Cl\u00e9ment Bonet",
      "Kimia Nadjahi",
      "Thibault S\u00e9journ\u00e9",
      "Kilian Fatras",
      "Nicolas Courty"
    ],
    "category": "cs.LG",
    "published_date": "2023-06-12 15:15:00+00:00",
    "updated_date": "2025-02-03 15:00:31+00:00",
    "arxiv_url": "http://arxiv.org/abs/2306.07176v2",
    "pdf_url": "https://arxiv.org/pdf/2306.07176v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2307.05703v2",
    "title": "Simple unbalanced optimal transport",
    "abstract": "We introduce and study a simple model capturing the main features of unbalanced optimal transport. It is based on equipping the conical extension of the group of all diffeomorphisms with a natural metric, which allows a Riemannian submersion to the space of volume forms of arbitrary total mass. We describe its finite-dimensional version and present a concise comparison study of the geometry, Hamiltonian features, and geodesics for this and other extensions. One of the corollaries of this approach is that along any geodesic the total mass evolves with constant acceleration, as an object's height in a constant buoyancy field.",
    "authors": [
      "Boris Khesin",
      "Klas Modin",
      "Luke Volk"
    ],
    "category": "math.DG",
    "published_date": "2023-07-11 18:11:49+00:00",
    "updated_date": "2024-01-24 08:11:43+00:00",
    "arxiv_url": "http://arxiv.org/abs/2307.05703v2",
    "pdf_url": "https://arxiv.org/pdf/2307.05703v2.pdf",
    "doi": "http://dx.doi.org/10.1093/imrn/rnae020",
    "journal_ref": "Int. Math. Res. Not. 2024 (2024) 8839-8855",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2501.04658v3",
    "title": "Quadratic-form Optimal Transport",
    "abstract": "We introduce the framework of quadratic-form optimal transport (QOT), whose transport cost has the form $\\iint c\\,\\mathrm{d}\\pi \\otimes\\mathrm{d}\\pi$ for some coupling $\\pi$ between two marginals. Interesting examples of quadratic-form transport cost and their optimization include inequality measurement, the variance of a bivariate function, covariance, Kendall's tau, the Gromov--Wasserstein distance, quadratic assignment problems, and quadratic regularization of classic optimal transport. QOT leads to substantially different mathematical structures compared to classic transport problems and many technical challenges. We illustrate the fundamental properties of QOT and provide several cases where explicit solutions are obtained. For a wide class of cost functions, including the rectangular cost functions, the QOT problem is solved by a new coupling called the diamond transport, whose copula is supported on a diamond in the unit square.",
    "authors": [
      "Ruodu Wang",
      "Zhenyuan Zhang"
    ],
    "category": "math.PR",
    "published_date": "2025-01-08 18:13:35+00:00",
    "updated_date": "2025-02-15 09:12:06+00:00",
    "arxiv_url": "http://arxiv.org/abs/2501.04658v3",
    "pdf_url": "https://arxiv.org/pdf/2501.04658v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2002.08717v2",
    "title": "The Directional Optimal Transport",
    "abstract": "We introduce a constrained optimal transport problem where origins $x$ can only be transported to destinations $y\\geq x$. Our statistical motivation is to describe the sharp upper bound for the variance of the treatment effect $Y-X$ given marginals when the effect is monotone, or $Y\\geq X$. We thus focus on supermodular costs (or submodular rewards) and introduce a coupling $P_{*}$ that is optimal for all such costs and yields the sharp bound. This coupling admits manifold characterizations -- geometric, order-theoretic, as optimal transport, through the cdf, and via the transport kernel -- that explain its structure and imply useful bounds. When the first marginal is atomless, $P_{*}$ is concentrated on the graphs of two maps which can be described in terms of the marginals, the second map arising due to the binding constraint.",
    "authors": [
      "Marcel Nutz",
      "Ruodu Wang"
    ],
    "category": "math.OC",
    "published_date": "2020-02-20 13:11:47+00:00",
    "updated_date": "2021-06-19 17:18:26+00:00",
    "arxiv_url": "http://arxiv.org/abs/2002.08717v2",
    "pdf_url": "https://arxiv.org/pdf/2002.08717v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1307.5551v1",
    "title": "Regularized Discrete Optimal Transport",
    "abstract": "This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks, which necessitate to take into account families of multimodal histograms, with large mass variation across modes.   The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or first order proximal splitting schemes.   The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across images color palettes. Furthermore, the regularization of the transport plan helps to remove colorization artifacts due to noise amplification.   We also extend this framework to the computation of barycenters of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter defines a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images.",
    "authors": [
      "Sira Ferradans",
      "Nicolas Papadakis",
      "Gabriel Peyr\u00e9",
      "Jean-Fran\u00e7ois Aujol"
    ],
    "category": "cs.CV",
    "published_date": "2013-07-21 17:55:10+00:00",
    "updated_date": "2013-07-21 17:55:10+00:00",
    "arxiv_url": "http://arxiv.org/abs/1307.5551v1",
    "pdf_url": "https://arxiv.org/pdf/1307.5551v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1903.01112v4",
    "title": "Quadratically regularized optimal transport",
    "abstract": "We investigate the problem of optimal transport in the so-called Kantorovich form, i.e. given two Radon measures on two compact sets, we seek an optimal transport plan which is another Radon measure on the product of the sets that has these two measures as marginals and minimizes a certain cost function. We consider quadratic regularization of the problem, which forces the optimal transport plan to be a square integrable function rather than a Radon measure. We derive the dual problem and show strong duality and existence of primal and dual solutions to the regularized problem. Then we derive two algorithms to solve the dual problem of the regularized problem: A Gauss-Seidel method and a semismooth quasi-Newton method and investigate both methods numerically. Our experiments show that the methods perform well even for small regularization parameters. Quadratic regularization is of interest since the resulting optimal transport plans are sparse, i.e. they have a small support (which is not the case for the often used entropic regularization where the optimal transport plan always has full measure).",
    "authors": [
      "Dirk A. Lorenz",
      "Paul Manns",
      "Christian Meyer"
    ],
    "category": "math.OC",
    "published_date": "2019-03-04 07:58:31+00:00",
    "updated_date": "2019-09-09 12:44:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/1903.01112v4",
    "pdf_url": "https://arxiv.org/pdf/1903.01112v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1911.04348v4",
    "title": "Semi-discrete optimal transport",
    "abstract": "In the current book I suggest an off-road path to the subject of optimal transport. I tried to avoid prior knowledge of analysis, PDE theory and functional analysis, as much as possible. Thus I concentrate on discrete and semi-discrete cases, and always assume compactness for the underlying spaces. However, some fundamental knowledge of measure theory and convexity is unavoidable. In order to make it as self-contained as possible I included an appendix with some basic definitions and results. I believe that any graduate student in mathematics, as well as advanced undergraduate students, can read and understand this book. Some chapters (in particular in Parts II\\&III ) can also be interesting for experts. Starting with the the most fundamental, fully discrete problem I attempted to place optimal transport as a particular case of the celebrated stable marriage problem. From there we proceed to the partition problem, which can be formulated as a transport from a continuous space to a discrete one. Applications to information theory and game theory (cooperative and non-cooperative) are introduced as well.   Finally, the general case of transport between two compact measure spaces is introduced as a coupling between two semi-discrete transports.",
    "authors": [
      "Gershon Wolansky"
    ],
    "category": "math.OC",
    "published_date": "2019-11-11 15:44:44+00:00",
    "updated_date": "2020-09-14 11:56:29+00:00",
    "arxiv_url": "http://arxiv.org/abs/1911.04348v4",
    "pdf_url": "https://arxiv.org/pdf/1911.04348v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2102.10909v4",
    "title": "Optimal Transport of Information",
    "abstract": "We study the general problem of Bayesian persuasion (optimal information design) with continuous actions and continuous state space in arbitrary dimensions. First, we show that with a finite signal space, the optimal information design is always given by a partition. Second, we take the limit of an infinite signal space and characterize the solution in terms of a Monge-Kantorovich optimal transport problem with an endogenous information transport cost. We use our novel approach to: 1. Derive necessary and sufficient conditions for optimality based on Bregman divergences for non-convex functions. 2. Compute exact bounds for the Hausdorff dimension of the support of an optimal policy. 3. Derive a non-linear, second-order partial differential equation whose solutions correspond to regular optimal policies. We illustrate the power of our approach by providing explicit solutions to several non-linear, multidimensional Bayesian persuasion problems.",
    "authors": [
      "Semyon Malamud",
      "Anna Cieslak",
      "Andreas Schrimpf"
    ],
    "category": "econ.GN",
    "published_date": "2021-02-22 11:28:36+00:00",
    "updated_date": "2021-03-09 13:43:29+00:00",
    "arxiv_url": "http://arxiv.org/abs/2102.10909v4",
    "pdf_url": "https://arxiv.org/pdf/2102.10909v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2201.06859v4",
    "title": "Grand-Canonical Optimal Transport",
    "abstract": "We study a generalization of the multi-marginal optimal transport problem, which has no fixed number of marginals $N$ and is inspired of statistical mechanics. It consists in optimizing a linear combination of the costs for all the possible $N$'s, while fixing a certain linear combination of the corresponding marginals.",
    "authors": [
      "Simone Di Marino",
      "Mathieu Lewin",
      "Luca Nenna"
    ],
    "category": "math.OC",
    "published_date": "2022-01-18 10:29:31+00:00",
    "updated_date": "2025-01-14 08:35:58+00:00",
    "arxiv_url": "http://arxiv.org/abs/2201.06859v4",
    "pdf_url": "https://arxiv.org/pdf/2201.06859v4.pdf",
    "doi": "http://dx.doi.org/10.1007/s00205-024-02080-x",
    "journal_ref": "Arch. Rat. Mech. Anal., vol. 249, art. 12 (2025)",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2206.13352v1",
    "title": "Constrained Mass Optimal Transport",
    "abstract": "Optimal mass transport, also known as the earth mover's problem, is an optimization problem with important applications in various disciplines, including economics, probability theory, fluid dynamics, cosmology and geophysics to cite a few. Optimal transport has also found successful applications in image registration, content-based image retrieval, and more generally in pattern recognition and machine learning as a way to measure dissimilarity among data. This paper introduces the problem of constrained optimal transport. The time-dependent formulation, more precisely, the fluid dynamics approach is used as a starting point from which the constrained problem is defined by imposing a soft constraint on the density and momentum fields or restricting them to a subset of curves that satisfy some prescribed conditions. A family of algorithms is introduced to solve a class of constrained saddle point problems, which has convexly constrained optimal transport on closed convex subsets of the Euclidean space as a special case. Convergence proofs and numerical results are presented.",
    "authors": [
      "Said Kerrache",
      "Yasushi Nakauchi"
    ],
    "category": "math.NA",
    "published_date": "2022-06-05 06:47:25+00:00",
    "updated_date": "2022-06-05 06:47:25+00:00",
    "arxiv_url": "http://arxiv.org/abs/2206.13352v1",
    "pdf_url": "https://arxiv.org/pdf/2206.13352v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2212.00133v6",
    "title": "Universal Neural Optimal Transport",
    "abstract": "Optimal Transport (OT) problems are a cornerstone of many applications, but solving them is computationally expensive. To address this problem, we propose UNOT (Universal Neural Optimal Transport), a novel framework capable of accurately predicting (entropic) OT distances and plans between discrete measures for a given cost function. UNOT builds on Fourier Neural Operators, a universal class of neural networks that map between function spaces and that are discretization-invariant, which enables our network to process measures of variable resolutions. The network is trained adversarially using a second, generating network and a self-supervised bootstrapping loss. We ground UNOT in an extensive theoretical framework. Through experiments on Euclidean and non-Euclidean domains, we show that our network not only accurately predicts OT distances and plans across a wide range of datasets, but also captures the geometry of the Wasserstein space correctly. Furthermore, we show that our network can be used as a state-of-the-art initialization for the Sinkhorn algorithm with speedups of up to $7.4\\times$, significantly outperforming existing approaches.",
    "authors": [
      "Jonathan Geuter",
      "Gregor Kornhardt",
      "Ingimar Tomasson",
      "Vaios Laschos"
    ],
    "category": "cs.LG",
    "published_date": "2022-11-30 21:56:09+00:00",
    "updated_date": "2025-06-12 12:03:45+00:00",
    "arxiv_url": "http://arxiv.org/abs/2212.00133v6",
    "pdf_url": "https://arxiv.org/pdf/2212.00133v6.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2308.11238v2",
    "title": "Distorted optimal transport",
    "abstract": "Classic optimal transport theory is formulated through minimizing the expected transport cost between two given distributions. We propose the framework of distorted optimal transport by minimizing a distorted expected cost, which is the cost under a non-linear expectation. This new formulation is motivated by concrete problems in decision theory, robust optimization, and risk management, and it has many distinct features compared to the classic theory. We choose simple cost functions and study different distortion functions and their implications on the optimal transport plan. We show that on the real line, the comonotonic coupling is optimal for the distorted optimal transport problem when the distortion function is convex and the cost function is submodular and monotone. Some forms of duality and uniqueness results are provided. For inverse-S-shaped distortion functions and linear cost, we obtain the unique form of optimal coupling for all marginal distributions, which turns out to have an interesting ``first comonotonic, then counter-monotonic\" dependence structure; for S-shaped distortion functions a similar structure is obtained. Our results highlight several challenges and features in distorted optimal transport, offering a new mathematical bridge between the fields of probability, decision theory, and risk management.",
    "authors": [
      "Haiyan Liu",
      "Bin Wang",
      "Ruodu Wang",
      "Sheng Chao Zhuang"
    ],
    "category": "math.OC",
    "published_date": "2023-08-22 07:25:51+00:00",
    "updated_date": "2025-05-17 21:30:05+00:00",
    "arxiv_url": "http://arxiv.org/abs/2308.11238v2",
    "pdf_url": "https://arxiv.org/pdf/2308.11238v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2505.12097v2",
    "title": "Proximal optimal transport divergences",
    "abstract": "We introduce the proximal optimal transport divergence, a novel discrepancy measure that interpolates between information divergences and optimal transport distances via an infimal convolution formulation. This divergence provides a principled foundation for optimal transport proximals and proximal optimization methods frequently used in generative modeling. We explore its mathematical properties, including smoothness, boundedness, and computational tractability, and establish connections to primal-dual formulations and adversarial learning. The proximal operator associated with the proximal optimal transport divergence can be interpreted as a transport map that pushes a reference distribution toward the optimal generative distribution, which approximates the target distribution that is only accessible through data samples. Building on the Benamou-Brenier dynamic formulation of classical optimal transport, we also establish a dynamic formulation for proximal OT divergences. The resulting dynamic formulation is a first order mean-field game whose optimality conditions are governed by a pair of nonlinear partial differential equations: a backward Hamilton-Jacobi equation and a forward continuity equation. Our framework generalizes existing approaches while offering new insights and computational tools for generative modeling, distributionally robust optimization, and gradient-based learning in probability spaces.",
    "authors": [
      "Ricardo Baptista",
      "Panagiota Birmpa",
      "Markos A. Katsoulakis",
      "Luc Rey-Bellet",
      "Benjamin J. Zhang"
    ],
    "category": "math.OC",
    "published_date": "2025-05-17 17:48:11+00:00",
    "updated_date": "2025-08-07 21:18:46+00:00",
    "arxiv_url": "http://arxiv.org/abs/2505.12097v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12097v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2411.05678v2",
    "title": "Relative Optimal Transport",
    "abstract": "We develop a theory of optimal transport relative to a distinguished subset, which acts as a reservoir of mass, allowing us to compare measures of different total variation. This relative transportation problem has an optimal solution and we obtain relative versions of the Kantorovich-Rubinstein norm, Wasserstein distance, Kantorovich-Rubinstein duality and Monge-Kantorovich duality. We also prove relative versions of the Riesz-Markov-Kakutani theorem, which connect the spaces of measures arising from the relative optimal transport problem to spaces of Lipschitz functions. For a boundedly compact Polish space, we show that our relative 1-finite real-valued Radon measures with relative Kantorovich-Rubinstein norm coincide with the sequentially order continuous dual of relative Lipschitz functions with the operator norm. As part of our work we develop a theory of Riesz cones that may be of independent interest.",
    "authors": [
      "Peter Bubenik",
      "Alex Elchesen"
    ],
    "category": "math.MG",
    "published_date": "2024-11-08 16:29:06+00:00",
    "updated_date": "2025-02-18 00:49:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/2411.05678v2",
    "pdf_url": "https://arxiv.org/pdf/2411.05678v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1304.6456v1",
    "title": "Insights into capacity constrained optimal transport",
    "abstract": "A variant of the classical optimal transportation problem is: among all joint measures with fixed marginals and which are dominated by a given density, find the optimal one. Existence and uniqueness of solutions to this variant were established in \\cite{KM11}. In the present manuscript, we expose an unexpected symmetry leading to the first explicit examples in two and more dimensions. These are inspired in part by simulations in one dimension which display singularities and topology and in part by two further developments: the identification of all extreme points in the feasible set, and a new approach to uniqueness based on constructing feasible perturbations.",
    "authors": [
      "Jonathan Korman",
      "Robert J. McCann"
    ],
    "category": "math.OC",
    "published_date": "2013-04-24 01:06:00+00:00",
    "updated_date": "2013-04-24 01:06:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/1304.6456v1",
    "pdf_url": "https://arxiv.org/pdf/1304.6456v1.pdf",
    "doi": "http://dx.doi.org/10.1073/pnas.1221333110",
    "journal_ref": "Proc. Natl. Acad. Sci. USA, 110 (2013) 10064-10067",
    "topic_category": "optimal_transport"
  },
  {
    "id": "1502.06839v1",
    "title": "Distribution functions, extremal limits and optimal transport",
    "abstract": "Encouraged by the study of extremal limits for sums of the form $$\\lim_{N\\to\\infty}\\frac{1 }{N}\\sum_{n=1}^N c(x_n,y_n)$$ with uniformly distributed sequences $\\{x_n\\},\\,\\{y_n\\}$ the following extremal problem is of interest $$\\max_{\\gamma}\\int_{[0,1]^2}c(x,y)\\gamma(dx,dy),$$ for probability measures $\\gamma$ on the unit square with uniform marginals, i.e., measures whose distribution function is a copula. The aim of this article is to relate this problem to combinatorial optimization and to the theory of optimal transport. Using different characterizations of maximizing $\\gamma$'s one can give alternative proofs of some results from the field of uniform distribution theory and beyond that treat additional questions. Finally, some applications to mathematical finance are addressed.",
    "authors": [
      "Maria Rita Iac\u00f2",
      "Stefan Thonhauser",
      "Robert F. Tichy"
    ],
    "category": "math.OC",
    "published_date": "2015-02-24 15:44:45+00:00",
    "updated_date": "2015-02-24 15:44:45+00:00",
    "arxiv_url": "http://arxiv.org/abs/1502.06839v1",
    "pdf_url": "https://arxiv.org/pdf/1502.06839v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1703.00289v4",
    "title": "A Multi-Objective Interpretation of Optimal Transport",
    "abstract": "This paper connects discrete optimal transport to a certain class of multi-objective optimization problems. In both settings, the decision variables can be organized into a matrix. In the multi-objective problem, the notion of Pareto efficiency is defined in terms of the objectives together with non-negativity constraints and with equality constraints that are specified in terms of column sums. A second set of equality constraints, defined in terms of row sums, is used to single out particular points in the Pareto efficient set which are referred to as \"balanced solutions\". Examples from several fields are shown in which this solution concept appears naturally. Balanced solutions are shown to be in one-to-one correspondence with solutions of optimal transport problems. As an example of the use of alternative interpretations, the computation of solutions via regularization is discussed.",
    "authors": [
      "Johannes M. Schumacher"
    ],
    "category": "math.OC",
    "published_date": "2017-03-01 13:30:12+00:00",
    "updated_date": "2017-12-01 15:27:30+00:00",
    "arxiv_url": "http://arxiv.org/abs/1703.00289v4",
    "pdf_url": "https://arxiv.org/pdf/1703.00289v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1704.00855v1",
    "title": "Population games and Discrete optimal transport",
    "abstract": "We propose a new evolutionary dynamics for population games with a discrete strategy set, inspired by the theory of optimal transport and Mean field games. The dynamics can be described as a Fokker-Planck equation on a discrete strategy set. The derived dynamics is the gradient flow of a free energy and the transition density equation of a Markov process. Such process provides models for the behavior of the individual players in population, which is myopic, greedy and irrational. The stability of the dynamics is governed by optimal transport metric, entropy and Fisher information.",
    "authors": [
      "Shui-Nee Chow",
      "Wuchen Li",
      "Jun Lu",
      "Haomin Zhou"
    ],
    "category": "math.OC",
    "published_date": "2017-04-04 02:43:05+00:00",
    "updated_date": "2017-04-04 02:43:05+00:00",
    "arxiv_url": "http://arxiv.org/abs/1704.00855v1",
    "pdf_url": "https://arxiv.org/pdf/1704.00855v1.pdf",
    "doi": "http://dx.doi.org/10.1007/s00332-018-9507-5",
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1808.01962v2",
    "title": "Semi-discrete unbalanced optimal transport and quantization",
    "abstract": "In this paper we study the class of optimal entropy-transport problems introduced by Liero, Mielke and Savar\\'e in Inventiones Mathematicae 211 in 2018. This class of unbalanced transport metrics allows for transport between measures of different total mass, unlike classical optimal transport where both measures must have the same total mass. In particular, we develop the theory for the important subclass of semi-discrete unbalanced transport problems, where one of the measures is diffuse (absolutely continuous with respect to the Lebesgue measure) and the other is discrete (a sum of Dirac masses). We characterize the optimal solutions and show they can be written in terms of generalized Laguerre diagrams. We use this to develop an efficient method for solving the semi-discrete unbalanced transport problem numerically. As an application we study the unbalanced quantization problem, where one looks for the best approximation of a diffuse measure by a discrete measure with respect to an unbalanced transport metric. We prove a type of crystallization result in two dimensions -- optimality of a locally triangular lattice with spatially varying density -- and compute the asymptotic quantization error as the number of Dirac masses tends to infinity.",
    "authors": [
      "David P. Bourne",
      "Bernhard Schmitzer",
      "Benedikt Wirth"
    ],
    "category": "math.OC",
    "published_date": "2018-08-06 15:24:35+00:00",
    "updated_date": "2024-07-16 07:19:27+00:00",
    "arxiv_url": "http://arxiv.org/abs/1808.01962v2",
    "pdf_url": "https://arxiv.org/pdf/1808.01962v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2009.02510v3",
    "title": "Optimal Transport between Gaussian Stationary Processes",
    "abstract": "We consider the optimal transport problem between multivariate Gaussian stationary stochastic processes. The transportation effort is the variance of the filtered discrepancy process. The main contribution of this technical note is to show that the corresponding solution leads to a weighted Hellinger distance between multivariate power spectral densities. Then, we propose a spectral estimation approach in the case of indirect measurements which is based on this distance.",
    "authors": [
      "Mattia Zorzi"
    ],
    "category": "math.OC",
    "published_date": "2020-09-05 10:27:26+00:00",
    "updated_date": "2021-01-10 10:10:50+00:00",
    "arxiv_url": "http://arxiv.org/abs/2009.02510v3",
    "pdf_url": "https://arxiv.org/pdf/2009.02510v3.pdf",
    "doi": null,
    "journal_ref": "IEEE Transactions on Automatic Control 2021",
    "topic_category": "optimal_transport"
  },
  {
    "id": "1307.7774v2",
    "title": "Dual potentials for capacity constrained optimal transport",
    "abstract": "Optimal transportation with capacity constraints, a variant of the well-known optimal transportation problem, is concerned with transporting one probability density $f \\in L^1(\\mathbb{R}^m)$ onto another one $g \\in L^1(\\mathbb{R}^n)$ so as to optimize a cost function $c \\in L^1(\\mathbb{R}^{m+n})$ while respecting the capacity constraints $0\\le h \\le \\bar h\\in L^\\infty(\\mathbb{R}^{m+n})$.   A linear programming duality theorem for this problem was first established by Levin. In this note, we prove under mild assumptions on the given data, the existence of a pair of $L^1$-functions optimizing the dual problem. Using these functions, which can be viewed as Lagrange multipliers to the marginal constraints $f$ and $g$, we characterize the solution $h$ of the primal problem. We expect these potentials to play a key role in any further analysis of $h$.   Moreover, starting from Levin's duality, we derive the classical Kantorovich duality for unconstrained optimal transport. In tandem with results obtained in our companion paper (arXiv:1309.3022), this amounts to a new and elementary proof of Kantorovich's duality.",
    "authors": [
      "Jonathan Korman",
      "Robert J. McCann",
      "Christian Seis"
    ],
    "category": "math.OC",
    "published_date": "2013-07-30 01:39:01+00:00",
    "updated_date": "2014-03-03 23:52:53+00:00",
    "arxiv_url": "http://arxiv.org/abs/1307.7774v2",
    "pdf_url": "https://arxiv.org/pdf/1307.7774v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1906.01333v3",
    "title": "Entropic regularization of continuous optimal transport problems",
    "abstract": "We analyze continuous optimal transport problems in the so-called Kantorovich form, where we seek a transport plan between two marginals that are probability measures on compact subsets of Euclidean space. We consider the case of regularization with the negative entropy with respect to the Lebesgue measure, which has attracted attention because it can be solved by the very simple Sinkhorn algorithm. We first analyze the regularized problem in the context of classical Fenchel duality and derive a strong duality result for a predual problem in the space of continuous functions. However, this problem may not admit a minimizer, which prevents obtaining primal-dual optimality conditions. We then show that the primal problem is naturally analyzed in the Orlicz space of functions with finite entropy in the sense that the entropically regularized problem admits a minimizer if and only if the marginals have finite entropy. We then derive a dual problem in the corresponding dual space, for which existence can be shown by purely variational arguments and primal-dual optimality conditions can be derived. For marginals that do not have finite entropy, we finally show Gamma-convergence of the regularized problem with smoothed marginals to the original Kantorovich problem.",
    "authors": [
      "Christian Clason",
      "Dirk A. Lorenz",
      "Hinrich Mahler",
      "Benedikt Wirth"
    ],
    "category": "math.OC",
    "published_date": "2019-06-04 10:50:13+00:00",
    "updated_date": "2020-06-15 14:33:37+00:00",
    "arxiv_url": "http://arxiv.org/abs/1906.01333v3",
    "pdf_url": "https://arxiv.org/pdf/1906.01333v3.pdf",
    "doi": "http://dx.doi.org/10.1016/j.jmaa.2020.124432",
    "journal_ref": "Journal of Mathematical Analysis and Applications 494 (2021),\n  124432",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2004.02294v3",
    "title": "Multimarginal Optimal Transport by Accelerated Alternating Minimization",
    "abstract": "We consider a multimarginal optimal transport, which includes as a particular case the Wasserstein barycenter problem. In this problem one has to find an optimal coupling between $m$ probability measures, which amounts to finding a tensor of the order $m$. We propose an accelerated method based on accelerated alternating minimization and estimate its complexity to find the approximate solution to the problem. We use entropic regularization with sufficiently small regularization parameter and apply accelerated alternating minimization to the dual problem. A novel primal-dual analysis is used to reconstruct the approximately optimal coupling tensor. Our algorithm exhibits a better computational complexity than the state-of-the-art methods for some regimes of the problem parameters.",
    "authors": [
      "Nazarii Tupitsa",
      "Pavel Dvurechensky",
      "Alexander Gasnikov",
      "C\u00e9sar A. Uribe"
    ],
    "category": "math.OC",
    "published_date": "2020-04-05 19:57:29+00:00",
    "updated_date": "2020-09-10 15:15:17+00:00",
    "arxiv_url": "http://arxiv.org/abs/2004.02294v3",
    "pdf_url": "https://arxiv.org/pdf/2004.02294v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1508.05216v3",
    "title": "Unbalanced Optimal Transport: Dynamic and Kantorovich Formulation",
    "abstract": "This article presents a new class of distances between arbitrary nonnegative Radon measures inspired by optimal transport. These distances are defined by two equivalent alternative formulations: (i) a dynamic formulation defining the distance as a geodesic distance over the space of measures (ii) a static \"Kantorovich\" formulation where the distance is the minimum of an optimization problem over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein-Fisher-Rao metric recently introduced independently by Chizat et al. and Kondratyev et al. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation.",
    "authors": [
      "Lenaic Chizat",
      "Gabriel Peyr\u00e9",
      "Bernhard Schmitzer",
      "Fran\u00e7ois-Xavier Vialard"
    ],
    "category": "math.OC",
    "published_date": "2015-08-21 09:00:08+00:00",
    "updated_date": "2019-02-09 12:44:58+00:00",
    "arxiv_url": "http://arxiv.org/abs/1508.05216v3",
    "pdf_url": "https://arxiv.org/pdf/1508.05216v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2205.00343v2",
    "title": "Distributional Uncertainty Propagation via Optimal Transport",
    "abstract": "This paper addresses the limitations of standard uncertainty models, e.g., robust (norm-bounded) and stochastic (one fixed distribution, e.g., Gaussian), and proposes to model uncertainty via Optimal Transport (OT) ambiguity sets. These constitute a very rich uncertainty model, which enjoys many desirable geometrical, statistical, and computational properties, and which: (1) naturally generalizes both robust and stochastic models, and (2) captures many additional real-world uncertainty phenomena (e.g., black swan events). Our contributions show that OT ambiguity sets are also analytically tractable: they propagate easily and intuitively through linear and nonlinear (possibly corrupted by noise) transformations, and the result of the propagation is again an OT ambiguity set or can be tightly upper bounded by an OT ambiguity set. In the context of dynamical systems, our results allow us to consider multiple sources of uncertainty (e.g., initial condition, additive noise, multiplicative noise) and to capture in closed-form, via an OT ambiguity set, the resulting uncertainty in the state at any future time. Our results are actionable, interpretable, and readily employable in a great variety of computationally tractable control and estimation formulations. To highlight this, we study three applications in trajectory planning, consensus algorithms, and least squares estimation. We conclude the paper with a list of exciting open problems enabled by our results.",
    "authors": [
      "Liviu Aolaritei",
      "Nicolas Lanzetti",
      "Hongruyu Chen",
      "Florian D\u00f6rfler"
    ],
    "category": "math.OC",
    "published_date": "2022-04-30 21:09:19+00:00",
    "updated_date": "2023-09-07 17:18:21+00:00",
    "arxiv_url": "http://arxiv.org/abs/2205.00343v2",
    "pdf_url": "https://arxiv.org/pdf/2205.00343v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2208.01958v2",
    "title": "Moment Constrained Optimal Transport for Control Applications",
    "abstract": "This paper concerns the application of techniques from optimal transport (OT) to mean field control, in which the probability measures of interest in OT correspond to empirical distributions associated with a large collection of controlled agents. The control objective of interest motivates a one-sided relaxation of OT, in which the first marginal is fixed and the second marginal is constrained to a moment class: a set of probability measures defined by generalized moment constraints. This relaxation is particularly interesting for control problems as it enables the coordination of agents without the need to know the desired distribution beforehand. The inclusion of an entropic regularizer is motivated by both computational considerations, and also to impose hard constraints on agent behavior. A computational approach inspired by the Sinkhorn algorithm is proposed to solve this problem. This new approach to distributed control is illustrated with an application of charging a fleet of electric vehicles while satisfying grid constraints. An online version is proposed and applied in a case study on the ElaadNL dataset containing 10,000 EV charging transactions in the Netherlands. This empirical validation demonstrates the effectiveness of the proposed approach to optimizing flexibility while respecting grid constraints.",
    "authors": [
      "Thomas Le Corre",
      "Ana Busic",
      "Sean Meyn"
    ],
    "category": "math.OC",
    "published_date": "2022-08-03 10:16:45+00:00",
    "updated_date": "2025-06-20 13:05:57+00:00",
    "arxiv_url": "http://arxiv.org/abs/2208.01958v2",
    "pdf_url": "https://arxiv.org/pdf/2208.01958v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2210.11368v2",
    "title": "Numerical Methods for Large-Scale Optimal Transport",
    "abstract": "The optimal transport (OT) problem is a classical optimization problem having the form of linear programming. Machine learning applications put forward new computational challenges in its solution. In particular, the OT problem defines a distance between real-world objects such as images, videos, texts, etc., modeled as probability distributions. In this case, the large dimension of the corresponding optimization problem does not allow applying classical methods such as network simplex or interior-point methods. This challenge was overcome by introducing entropic regularization and using the efficient Sinkhorn's algorithm to solve the regularized problem. A flexible alternative is the accelerated primal-dual gradient method, which can use any strongly-convex regularization. We discuss these algorithms and other related problems such as approximating the Wasserstein barycenter together with efficient algorithms for its solution, including decentralized distributed algorithms.",
    "authors": [
      "Nazarii Tupitsa",
      "Pavel Dvurechensky",
      "Darina Dvinskikh",
      "Alexander Gasnikov"
    ],
    "category": "math.OC",
    "published_date": "2022-10-20 16:10:05+00:00",
    "updated_date": "2022-10-24 10:57:57+00:00",
    "arxiv_url": "http://arxiv.org/abs/2210.11368v2",
    "pdf_url": "https://arxiv.org/pdf/2210.11368v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2305.02410v1",
    "title": "Entropic regularisation of unbalanced optimal transportation problems",
    "abstract": "We develop a mathematical theory of entropic regularisation of unbalanced optimal transport problems. Focusing on static formulation and relying on the formalism developed for the unregularised case, we show that unbalanced optimal transport problems can be regularised in two qualitatively distinct ways - either on the original space or on the extended space. We derive several reformulations of the two regularised problems and in particular introduce the idea of a regularised induced marginal perspective cost function allowing us to derive an extended space formulation of the original space regularisation. We also prove convergence to the unregularised problem in the case of the extended space regularisation and discuss on-going work on deriving a unified framework based on higher order liftings in which both regularisations can be directly compared. We also briefly touch upon how these concepts translate to the corresponding dynamic formulations and provide evidence why the extended space regularisation should be preferred. This is a preliminary version of the manuscript, to be updated in the near future.",
    "authors": [
      "Maciej Buze",
      "Manh Hong Duong"
    ],
    "category": "math.OC",
    "published_date": "2023-05-03 20:09:00+00:00",
    "updated_date": "2023-05-03 20:09:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/2305.02410v1",
    "pdf_url": "https://arxiv.org/pdf/2305.02410v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2307.00321v2",
    "title": "Algorithms for Euclidean-regularised Optimal Transport",
    "abstract": "This paper addresses the Optimal Transport problem, which is regularized by the square of Euclidean $\\ell_2$-norm. It offers theoretical guarantees regarding the iteration complexities of the Sinkhorn--Knopp algorithm, Accelerated Gradient Descent, Accelerated Alternating Minimisation, and Coordinate Linear Variance Reduction algorithms. Furthermore, the paper compares the practical efficiency of these methods and their counterparts when applied to the entropy-regularized Optimal Transport problem. This comparison is conducted through numerical experiments carried out on the MNIST dataset.",
    "authors": [
      "Dmitry A. Pasechnyuk",
      "Michael Persiianov",
      "Pavel Dvurechensky",
      "Alexander Gasnikov"
    ],
    "category": "math.OC",
    "published_date": "2023-07-01 12:14:18+00:00",
    "updated_date": "2023-08-28 08:44:54+00:00",
    "arxiv_url": "http://arxiv.org/abs/2307.00321v2",
    "pdf_url": "https://arxiv.org/pdf/2307.00321v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2310.02510v2",
    "title": "Interior point method in tensor optimal transport",
    "abstract": "We study a tensor optimal transport (TOT) problem for $d\\ge 2$ discrete measures. This is a linear programming problem on $d$-tensors. We introduces an interior point method (ipm) for $d$-TOT with a corresponding barrier function. Using a \"short-step\" ipm following central path within $\\varepsilon$ precision we estimate the number of iterations.",
    "authors": [
      "Shmuel Friedland"
    ],
    "category": "math.OC",
    "published_date": "2023-10-04 01:07:40+00:00",
    "updated_date": "2023-10-29 18:35:47+00:00",
    "arxiv_url": "http://arxiv.org/abs/2310.02510v2",
    "pdf_url": "https://arxiv.org/pdf/2310.02510v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2312.06811v2",
    "title": "Optimal reinsurance from an optimal transport perspective",
    "abstract": "We use the randomization idea and proof techniques from optimal transport to study optimal reinsurance problems. We start by providing conditions for a class of problems that allow us to characterize the support of optimal treaties, and show how this can be used to deduce the shape of the optimal contract, reducing the task to an optimization problem with finitely many constraints, for which standard techniques can be applied. For a more general class of problems, we regard the optimal reinsurance problem as an iterated optimal transport problem between a (known) initial risk exposure of the insurer and an (unknown) resulting risk exposure of the reinsurer. The proposed approach provides a general framework that encompasses many reinsurance problems, which we illustrate in several concrete examples, providing alternative proofs to classical optimal reinsurance results, as well as establishing new optimality results, some of which contain optimal treaties that involve external randomness.",
    "authors": [
      "Beatrice Acciaio",
      "Hansj\u00f6rg Albrecher",
      "Brandon Garc\u00eda Flores"
    ],
    "category": "math.OC",
    "published_date": "2023-12-11 19:55:58+00:00",
    "updated_date": "2024-11-01 16:21:51+00:00",
    "arxiv_url": "http://arxiv.org/abs/2312.06811v2",
    "pdf_url": "https://arxiv.org/pdf/2312.06811v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2201.12220v3",
    "title": "Neural Optimal Transport",
    "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation.",
    "authors": [
      "Alexander Korotin",
      "Daniil Selikhanovych",
      "Evgeny Burnaev"
    ],
    "category": "cs.LG",
    "published_date": "2022-01-28 16:24:13+00:00",
    "updated_date": "2023-03-01 13:38:35+00:00",
    "arxiv_url": "http://arxiv.org/abs/2201.12220v3",
    "pdf_url": "https://arxiv.org/pdf/2201.12220v3.pdf",
    "doi": null,
    "journal_ref": "The 11th International Conference on Learning Representations\n  (ICLR 2023)",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2503.05087v1",
    "title": "Partial Distribution Alignment via Adaptive Optimal Transport",
    "abstract": "To remedy the drawbacks of full-mass or fixed-mass constraints in classical optimal transport, we propose adaptive optimal transport which is distinctive from the classical optimal transport in its ability of adaptive-mass preserving. It aims to answer the mathematical problem of how to transport the probability mass adaptively between probability distributions, which is a fundamental topic in various areas of artificial intelligence. Adaptive optimal transport is able to transfer mass adaptively in the light of the intrinsic structure of the problem itself. The theoretical results shed light on the adaptive mechanism of mass transportation. Furthermore, we instantiate the adaptive optimal transport in machine learning application to align source and target distributions partially and adaptively by respecting the ubiquity of noises, outliers, and distribution shifts in the data. The experiment results on the domain adaptation benchmarks show that the proposed method significantly outperforms the state-of-the-art algorithms.",
    "authors": [
      "Pei Yang",
      "Qi Tan",
      "Guihua Wen"
    ],
    "category": "cs.LG",
    "published_date": "2025-03-07 02:13:04+00:00",
    "updated_date": "2025-03-07 02:13:04+00:00",
    "arxiv_url": "http://arxiv.org/abs/2503.05087v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05087v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1609.04495v1",
    "title": "Tsallis Regularized Optimal Transport and Ecological Inference",
    "abstract": "Optimal transport is a powerful framework for computing distances between probability distributions. We unify the two main approaches to optimal transport, namely Monge-Kantorovitch and Sinkhorn-Cuturi, into what we define as Tsallis regularized optimal transport (\\trot). \\trot~interpolates a rich family of distortions from Wasserstein to Kullback-Leibler, encompassing as well Pearson, Neyman and Hellinger divergences, to name a few. We show that metric properties known for Sinkhorn-Cuturi generalize to \\trot, and provide efficient algorithms for finding the optimal transportation plan with formal convergence proofs. We also present the first application of optimal transport to the problem of ecological inference, that is, the reconstruction of joint distributions from their marginals, a problem of large interest in the social sciences. \\trot~provides a convenient framework for ecological inference by allowing to compute the joint distribution --- that is, the optimal transportation plan itself --- when side information is available, which is \\textit{e.g.} typically what census represents in political science. Experiments on data from the 2012 US presidential elections display the potential of \\trot~in delivering a faithful reconstruction of the joint distribution of ethnic groups and voter preferences.",
    "authors": [
      "Boris Muzellec",
      "Richard Nock",
      "Giorgio Patrini",
      "Frank Nielsen"
    ],
    "category": "cs.LG",
    "published_date": "2016-09-15 02:30:10+00:00",
    "updated_date": "2016-09-15 02:30:10+00:00",
    "arxiv_url": "http://arxiv.org/abs/1609.04495v1",
    "pdf_url": "https://arxiv.org/pdf/1609.04495v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2210.12288v2",
    "title": "Learning Ultrametric Trees for Optimal Transport Regression",
    "abstract": "Optimal transport provides a metric which quantifies the dissimilarity between probability measures. For measures supported in discrete metric spaces, finding the optimal transport distance has cubic time complexity in the size of the space. However, measures supported on trees admit a closed-form optimal transport that can be computed in linear time. In this paper, we aim to find an optimal tree structure for a given discrete metric space so that the tree-Wasserstein distance approximates the optimal transport distance in the original space. One of our key ideas is to cast the problem in ultrametric spaces. This helps us optimize over the space of ultrametric trees -- a mixed-discrete and continuous optimization problem -- via projected gradient decent over the space of ultrametric matrices. During optimization, we project the parameters to the ultrametric space via a hierarchical minimum spanning tree algorithm, equivalent to the closest projection to ultrametrics under the supremum norm. Experimental results on real datasets show that our approach outperforms previous approaches (e.g. Flowtree, Quadtree) in approximating optimal transport distances. Finally, experiments on synthetic data generated on ground truth trees show that our algorithm can accurately uncover the underlying trees.",
    "authors": [
      "Samantha Chen",
      "Puoya Tabaghi",
      "Yusu Wang"
    ],
    "category": "cs.LG",
    "published_date": "2022-10-21 22:54:42+00:00",
    "updated_date": "2024-01-26 22:03:58+00:00",
    "arxiv_url": "http://arxiv.org/abs/2210.12288v2",
    "pdf_url": "https://arxiv.org/pdf/2210.12288v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2110.01446v1",
    "title": "Label Propagation Through Optimal Transport",
    "abstract": "In this paper, we tackle the transductive semi-supervised learning problem that aims to obtain label predictions for the given unlabeled data points according to Vapnik's principle. Our proposed approach is based on optimal transport, a mathematical theory that has been successfully used to address various machine learning problems, and is starting to attract renewed interest in semi-supervised learning community. The proposed approach, Optimal Transport Propagation (OTP), performs in an incremental process, label propagation through the edges of a complete bipartite edge-weighted graph, whose affinity matrix is constructed from the optimal transport plan between empirical measures defined on labeled and unlabeled data. OTP ensures a high degree of predictions certitude by controlling the propagation process using a certainty score based on Shannon's entropy. We also provide a convergence analysis of our algorithm. Experiments task show the superiority of the proposed approach over the state-of-the-art. We make our code publicly available.",
    "authors": [
      "Mourad El Hamri",
      "Youn\u00e8s Bennani",
      "Issam Falih"
    ],
    "category": "cs.LG",
    "published_date": "2021-10-01 11:25:55+00:00",
    "updated_date": "2021-10-01 11:25:55+00:00",
    "arxiv_url": "http://arxiv.org/abs/2110.01446v1",
    "pdf_url": "https://arxiv.org/pdf/2110.01446v1.pdf",
    "doi": "http://dx.doi.org/10.1109/IJCNN52387.2021.9533521",
    "journal_ref": "2021 International Joint Conference on Neural Networks (IJCNN)",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2309.15603v1",
    "title": "Distill Knowledge in Multi-task Reinforcement Learning with   Optimal-Transport Regularization",
    "abstract": "In multi-task reinforcement learning, it is possible to improve the data efficiency of training agents by transferring knowledge from other different but related tasks. Because the experiences from different tasks are usually biased toward the specific task goals. Traditional methods rely on Kullback-Leibler regularization to stabilize the transfer of knowledge from one task to the others. In this work, we explore the direction of replacing the Kullback-Leibler divergence with a novel Optimal transport-based regularization. By using the Sinkhorn mapping, we can approximate the Optimal transport distance between the state distribution of tasks. The distance is then used as an amortized reward to regularize the amount of sharing information. We experiment our frameworks on several grid-based navigation multi-goal to validate the effectiveness of the approach. The results show that our added Optimal transport-based rewards are able to speed up the learning process of agents and outperforms several baselines on multi-task learning.",
    "authors": [
      "Bang Giang Le",
      "Viet Cuong Ta"
    ],
    "category": "cs.LG",
    "published_date": "2023-09-27 12:06:34+00:00",
    "updated_date": "2023-09-27 12:06:34+00:00",
    "arxiv_url": "http://arxiv.org/abs/2309.15603v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15603v1.pdf",
    "doi": "http://dx.doi.org/10.1109/KSE56063.2022.9953750",
    "journal_ref": "2022 14th International Conference on Knowledge and Systems\n  Engineering (KSE), Nha Trang, Vietnam, 2022, pp. 1-6,",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2310.06002v1",
    "title": "LCOT: Linear circular optimal transport",
    "abstract": "The optimal transport problem for measures supported on non-Euclidean spaces has recently gained ample interest in diverse applications involving representation learning. In this paper, we focus on circular probability measures, i.e., probability measures supported on the unit circle, and introduce a new computationally efficient metric for these measures, denoted as Linear Circular Optimal Transport (LCOT). The proposed metric comes with an explicit linear embedding that allows one to apply Machine Learning (ML) algorithms to the embedded measures and seamlessly modify the underlying metric for the ML algorithm to LCOT. We show that the proposed metric is rooted in the Circular Optimal Transport (COT) and can be considered the linearization of the COT metric with respect to a fixed reference measure. We provide a theoretical analysis of the proposed metric and derive the computational complexities for pairwise comparison of circular probability measures. Lastly, through a set of numerical experiments, we demonstrate the benefits of LCOT in learning representations of circular measures.",
    "authors": [
      "Rocio Diaz Martin",
      "Ivan Medri",
      "Yikun Bai",
      "Xinran Liu",
      "Kangbai Yan",
      "Gustavo K. Rohde",
      "Soheil Kolouri"
    ],
    "category": "cs.LG",
    "published_date": "2023-10-09 14:37:56+00:00",
    "updated_date": "2023-10-09 14:37:56+00:00",
    "arxiv_url": "http://arxiv.org/abs/2310.06002v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06002v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2504.03188v2",
    "title": "Pairwise Optimal Transports for Training All-to-All Flow-Based Condition   Transfer Model",
    "abstract": "In this paper, we propose a flow-based method for learning all-to-all transfer maps among conditional distributions that approximates pairwise optimal transport. The proposed method addresses the challenge of handling the case of continuous conditions, which often involve a large set of conditions with sparse empirical observations per condition. We introduce a novel cost function that enables simultaneous learning of optimal transports for all pairs of conditional distributions. Our method is supported by a theoretical guarantee that, in the limit, it converges to the pairwise optimal transports among infinite pairs of conditional distributions. The learned transport maps are subsequently used to couple data points in conditional flow matching. We demonstrate the effectiveness of this method on synthetic and benchmark datasets, as well as on chemical datasets in which continuous physical properties are defined as conditions.",
    "authors": [
      "Kotaro Ikeda",
      "Masanori Koyama",
      "Jinzhe Zhang",
      "Kohei Hayashi",
      "Kenji Fukumizu"
    ],
    "category": "cs.LG",
    "published_date": "2025-04-04 05:32:54+00:00",
    "updated_date": "2025-05-27 05:58:24+00:00",
    "arxiv_url": "http://arxiv.org/abs/2504.03188v2",
    "pdf_url": "https://arxiv.org/pdf/2504.03188v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2507.13191v1",
    "title": "GradNetOT: Learning Optimal Transport Maps with GradNets",
    "abstract": "Monotone gradient functions play a central role in solving the Monge formulation of the optimal transport problem, which arises in modern applications ranging from fluid dynamics to robot swarm control. When the transport cost is the squared Euclidean distance, Brenier's theorem guarantees that the unique optimal map is the gradient of a convex function, namely a monotone gradient map, and it satisfies a Monge-Amp\\`ere equation. In [arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks (mGradNets), neural networks that directly parameterize the space of monotone gradient maps. In this work, we leverage mGradNets to directly learn the optimal transport mapping by minimizing a training loss function defined using the Monge-Amp\\`ere equation. We empirically show that the structural bias of mGradNets facilitates the learning of optimal transport maps and employ our method for a robot swarm control problem.",
    "authors": [
      "Shreyas Chaudhari",
      "Srinivasa Pranav",
      "Jos\u00e9 M. F. Moura"
    ],
    "category": "cs.LG",
    "published_date": "2025-07-17 14:59:24+00:00",
    "updated_date": "2025-07-17 14:59:24+00:00",
    "arxiv_url": "http://arxiv.org/abs/2507.13191v1",
    "pdf_url": "https://arxiv.org/pdf/2507.13191v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2410.14069v1",
    "title": "Rethinking Optimal Transport in Offline Reinforcement Learning",
    "abstract": "We propose a novel algorithm for offline reinforcement learning using optimal transport. Typically, in offline reinforcement learning, the data is provided by various experts and some of them can be sub-optimal. To extract an efficient policy, it is necessary to \\emph{stitch} the best behaviors from the dataset. To address this problem, we rethink offline reinforcement learning as an optimal transportation problem. And based on this, we present an algorithm that aims to find a policy that maps states to a \\emph{partial} distribution of the best expert actions for each given state. We evaluate the performance of our algorithm on continuous control problems from the D4RL suite and demonstrate improvements over existing methods.",
    "authors": [
      "Arip Asadulaev",
      "Rostislav Korst",
      "Alexander Korotin",
      "Vage Egiazarian",
      "Andrey Filchenkov",
      "Evgeny Burnaev"
    ],
    "category": "cs.LG",
    "published_date": "2024-10-17 22:36:43+00:00",
    "updated_date": "2024-10-17 22:36:43+00:00",
    "arxiv_url": "http://arxiv.org/abs/2410.14069v1",
    "pdf_url": "https://arxiv.org/pdf/2410.14069v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2203.10453v1",
    "title": "Fine-Tuning Graph Neural Networks via Graph Topology induced Optimal   Transport",
    "abstract": "Recently, the pretrain-finetuning paradigm has attracted tons of attention in graph learning community due to its power of alleviating the lack of labels problem in many real-world applications. Current studies use existing techniques, such as weight constraint, representation constraint, which are derived from images or text data, to transfer the invariant knowledge from the pre-train stage to fine-tuning stage. However, these methods failed to preserve invariances from graph structure and Graph Neural Network (GNN) style models. In this paper, we present a novel optimal transport-based fine-tuning framework called GTOT-Tuning, namely, Graph Topology induced Optimal Transport fine-Tuning, for GNN style backbones. GTOT-Tuning is required to utilize the property of graph data to enhance the preservation of representation produced by fine-tuned networks. Toward this goal, we formulate graph local knowledge transfer as an Optimal Transport (OT) problem with a structural prior and construct the GTOT regularizer to constrain the fine-tuned model behaviors. By using the adjacency relationship amongst nodes, the GTOT regularizer achieves node-level optimal transport procedures and reduces redundant transport procedures, resulting in efficient knowledge transfer from the pre-trained models. We evaluate GTOT-Tuning on eight downstream tasks with various GNN backbones and demonstrate that it achieves state-of-the-art fine-tuning performance for GNNs.",
    "authors": [
      "Jiying Zhang",
      "Xi Xiao",
      "Long-Kai Huang",
      "Yu Rong",
      "Yatao Bian"
    ],
    "category": "cs.LG",
    "published_date": "2022-03-20 04:41:17+00:00",
    "updated_date": "2022-03-20 04:41:17+00:00",
    "arxiv_url": "http://arxiv.org/abs/2203.10453v1",
    "pdf_url": "https://arxiv.org/pdf/2203.10453v1.pdf",
    "doi": null,
    "journal_ref": "IJCAI2022",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2106.01954v2",
    "title": "Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2   Benchmark",
    "abstract": "Despite the recent popularity of neural network-based solvers for optimal transport (OT), there is no standard quantitative way to evaluate their performance. In this paper, we address this issue for quadratic-cost transport -- specifically, computation of the Wasserstein-2 distance, a commonly-used formulation of optimal transport in machine learning. To overcome the challenge of computing ground truth transport maps between continuous measures needed to assess these solvers, we use input-convex neural networks (ICNN) to construct pairs of measures whose ground truth OT maps can be obtained analytically. This strategy yields pairs of continuous benchmark measures in high-dimensional spaces such as spaces of images. We thoroughly evaluate existing optimal transport solvers using these benchmark measures. Even though these solvers perform well in downstream tasks, many do not faithfully recover optimal transport maps. To investigate the cause of this discrepancy, we further test the solvers in a setting of image generation. Our study reveals crucial limitations of existing solvers and shows that increased OT accuracy does not necessarily correlate to better results downstream.",
    "authors": [
      "Alexander Korotin",
      "Lingxiao Li",
      "Aude Genevay",
      "Justin Solomon",
      "Alexander Filippov",
      "Evgeny Burnaev"
    ],
    "category": "cs.LG",
    "published_date": "2021-06-03 15:59:28+00:00",
    "updated_date": "2021-10-25 10:24:41+00:00",
    "arxiv_url": "http://arxiv.org/abs/2106.01954v2",
    "pdf_url": "https://arxiv.org/pdf/2106.01954v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1708.02469v1",
    "title": "Multiscale Strategies for Computing Optimal Transport",
    "abstract": "This paper presents a multiscale approach to efficiently compute approximate optimal transport plans between point sets. It is particularly well-suited for point sets that are in high-dimensions, but are close to being intrinsically low-dimensional. The approach is based on an adaptive multiscale decomposition of the point sets. The multiscale decomposition yields a sequence of optimal transport problems, that are solved in a top-to-bottom fashion from the coarsest to the finest scale. We provide numerical evidence that this multiscale approach scales approximately linearly, in time and memory, in the number of nodes, instead of quadratically or worse for a direct solution. Empirically, the multiscale approach results in less than one percent relative error in the objective function. Furthermore, the multiscale plans constructed are of interest by themselves as they may be used to introduce novel features and notions of distances between point sets. An analysis of sets of brain MRI based on optimal transport distances illustrates the effectiveness of the proposed method on a real world data set. The application demonstrates that multiscale optimal transport distances have the potential to improve on state-of-the-art metrics currently used in computational anatomy.",
    "authors": [
      "Samuel Gerber",
      "Mauro Maggioni"
    ],
    "category": "cs.LG",
    "published_date": "2017-08-08 12:54:27+00:00",
    "updated_date": "2017-08-08 12:54:27+00:00",
    "arxiv_url": "http://arxiv.org/abs/1708.02469v1",
    "pdf_url": "https://arxiv.org/pdf/1708.02469v1.pdf",
    "doi": null,
    "journal_ref": "Journal of Machine Learning Research 18 (2017): 1-32",
    "topic_category": "optimal_transport"
  },
  {
    "id": "2110.03237v4",
    "title": "Score-based Generative Neural Networks for Large-Scale Optimal Transport",
    "abstract": "We consider the fundamental problem of sampling the optimal transport coupling between given source and target distributions. In certain cases, the optimal transport plan takes the form of a one-to-one mapping from the source support to the target support, but learning or even approximating such a map is computationally challenging for large and high-dimensional datasets due to the high cost of linear programming routines and an intrinsic curse of dimensionality. We study instead the Sinkhorn problem, a regularized form of optimal transport whose solutions are couplings between the source and the target distribution. We introduce a novel framework for learning the Sinkhorn coupling between two distributions in the form of a score-based generative model. Conditioned on source data, our procedure iterates Langevin Dynamics to sample target data according to the regularized optimal coupling. Key to this approach is a neural network parametrization of the Sinkhorn problem, and we prove convergence of gradient descent with respect to network parameters in this formulation. We demonstrate its empirical success on a variety of large scale optimal transport tasks.",
    "authors": [
      "Max Daniels",
      "Tyler Maunu",
      "Paul Hand"
    ],
    "category": "cs.LG",
    "published_date": "2021-10-07 07:45:39+00:00",
    "updated_date": "2022-01-25 15:40:17+00:00",
    "arxiv_url": "http://arxiv.org/abs/2110.03237v4",
    "pdf_url": "https://arxiv.org/pdf/2110.03237v4.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2205.15424v2",
    "title": "Connecting adversarial attacks and optimal transport for domain   adaptation",
    "abstract": "We present a novel algorithm for domain adaptation using optimal transport. In domain adaptation, the goal is to adapt a classifier trained on the source domain samples to the target domain. In our method, we use optimal transport to map target samples to the domain named source fiction. This domain differs from the source but is accurately classified by the source domain classifier. Our main idea is to generate a source fiction by c-cyclically monotone transformation over the target domain. If samples with the same labels in two domains are c-cyclically monotone, the optimal transport map between these domains preserves the class-wise structure, which is the main goal of domain adaptation. To generate a source fiction domain, we propose an algorithm that is based on our finding that adversarial attacks are a c-cyclically monotone transformation of the dataset. We conduct experiments on Digits and Modern Office-31 datasets and achieve improvement in performance for simple discrete optimal transport solvers for all adaptation tasks.",
    "authors": [
      "Arip Asadulaev",
      "Vitaly Shutov",
      "Alexander Korotin",
      "Alexander Panfilov",
      "Andrey Filchenkov"
    ],
    "category": "cs.LG",
    "published_date": "2022-05-30 20:45:55+00:00",
    "updated_date": "2022-06-04 15:02:01+00:00",
    "arxiv_url": "http://arxiv.org/abs/2205.15424v2",
    "pdf_url": "https://arxiv.org/pdf/2205.15424v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2206.14928v2",
    "title": "Manifold Interpolating Optimal-Transport Flows for Trajectory Inference",
    "abstract": "We present a method called Manifold Interpolating Optimal-Transport Flow (MIOFlow) that learns stochastic, continuous population dynamics from static snapshot samples taken at sporadic timepoints. MIOFlow combines dynamic models, manifold learning, and optimal transport by training neural ordinary differential equations (Neural ODE) to interpolate between static population snapshots as penalized by optimal transport with manifold ground distance. Further, we ensure that the flow follows the geometry by operating in the latent space of an autoencoder that we call a geodesic autoencoder (GAE). In GAE the latent space distance between points is regularized to match a novel multiscale geodesic distance on the data manifold that we define. We show that this method is superior to normalizing flows, Schr\\\"odinger bridges and other generative models that are designed to flow from noise to data in terms of interpolating between populations. Theoretically, we link these trajectories with dynamic optimal transport. We evaluate our method on simulated data with bifurcations and merges, as well as scRNA-seq data from embryoid body differentiation, and acute myeloid leukemia treatment.",
    "authors": [
      "Guillaume Huguet",
      "D. S. Magruder",
      "Alexander Tong",
      "Oluwadamilola Fasina",
      "Manik Kuchroo",
      "Guy Wolf",
      "Smita Krishnaswamy"
    ],
    "category": "cs.LG",
    "published_date": "2022-06-29 22:19:03+00:00",
    "updated_date": "2022-11-03 12:45:37+00:00",
    "arxiv_url": "http://arxiv.org/abs/2206.14928v2",
    "pdf_url": "https://arxiv.org/pdf/2206.14928v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2405.15252v2",
    "title": "Accelerating 3D Molecule Generation via Jointly Geometric Optimal   Transport",
    "abstract": "This paper proposes a new 3D molecule generation framework, called GOAT, for fast and effective 3D molecule generation based on the flow-matching optimal transport objective. Specifically, we formulate a geometric transport formula for measuring the cost of mapping multi-modal features (e.g., continuous atom coordinates and categorical atom types) between a base distribution and a target data distribution. Our formula is solved within a joint, equivariant, and smooth representation space. This is achieved by transforming the multi-modal features into a continuous latent space with equivariant networks. In addition, we find that identifying optimal distributional coupling is necessary for fast and effective transport between any two distributions. We further propose a mechanism for estimating and purifying optimal coupling to train the flow model with optimal transport. By doing so, GOAT can turn arbitrary distribution couplings into new deterministic couplings, leading to an estimated optimal transport plan for fast 3D molecule generation. The purification filters out the subpar molecules to ensure the ultimate generation quality. We theoretically and empirically prove that the proposed optimal coupling estimation and purification yield transport plan with non-increasing cost. Finally, extensive experiments show that GOAT enjoys the efficiency of solving geometric optimal transport, leading to a double speedup compared to the sub-optimal method while achieving the best generation quality regarding validity, uniqueness, and novelty. The code is available at https://github.com/WanyuGroup/ICLR2025-GOAT.",
    "authors": [
      "Haokai Hong",
      "Wanyu Lin",
      "Kay Chen Tan"
    ],
    "category": "cs.LG",
    "published_date": "2024-05-24 06:22:01+00:00",
    "updated_date": "2025-03-02 14:10:09+00:00",
    "arxiv_url": "http://arxiv.org/abs/2405.15252v2",
    "pdf_url": "https://arxiv.org/pdf/2405.15252v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2504.02083v1",
    "title": "Measuring the Data",
    "abstract": "Measuring the Data analytically finds the intrinsic manifold in big data. First, Optimal Transport generates the tangent space at each data point from which the intrinsic dimension is revealed. Then, the Koopman Dimensionality Reduction procedure derives a nonlinear transformation from the data to the intrinsic manifold. Measuring the data procedure is presented here, backed up with encouraging results.",
    "authors": [
      "Ido Cohen"
    ],
    "category": "cs.LG",
    "published_date": "2025-04-02 19:43:08+00:00",
    "updated_date": "2025-04-02 19:43:08+00:00",
    "arxiv_url": "http://arxiv.org/abs/2504.02083v1",
    "pdf_url": "https://arxiv.org/pdf/2504.02083v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "1905.00158v3",
    "title": "On Scalable and Efficient Computation of Large Scale Optimal Transport",
    "abstract": "Optimal Transport (OT) naturally arises in many machine learning applications, yet the heavy computational burden limits its wide-spread uses. To address the scalability issue, we propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms. We also show that we can recover the density of the optimal transport plan using neural ordinary differential equations. Numerical experiments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently sample from the optimal transport plan, which benefits downstream applications such as domain adaptation.",
    "authors": [
      "Yujia Xie",
      "Minshuo Chen",
      "Haoming Jiang",
      "Tuo Zhao",
      "Hongyuan Zha"
    ],
    "category": "cs.LG",
    "published_date": "2019-05-01 01:32:52+00:00",
    "updated_date": "2019-06-24 19:22:14+00:00",
    "arxiv_url": "http://arxiv.org/abs/1905.00158v3",
    "pdf_url": "https://arxiv.org/pdf/1905.00158v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2008.02995v3",
    "title": "A Review on Modern Computational Optimal Transport Methods with   Applications in Biomedical Research",
    "abstract": "Optimal transport has been one of the most exciting subjects in mathematics, starting from the 18th century. As a powerful tool to transport between two probability measures, optimal transport methods have been reinvigorated nowadays in a remarkable proliferation of modern data science applications. To meet the big data challenges, various computational tools have been developed in the recent decade to accelerate the computation for optimal transport methods. In this review, we present some cutting-edge computational optimal transport methods with a focus on the regularization-based methods and the projection-based methods. We discuss their real-world applications in biomedical research.",
    "authors": [
      "Jingyi Zhang",
      "Wenxuan Zhong",
      "Ping Ma"
    ],
    "category": "stat.ML",
    "published_date": "2020-08-07 05:33:54+00:00",
    "updated_date": "2021-05-20 09:39:21+00:00",
    "arxiv_url": "http://arxiv.org/abs/2008.02995v3",
    "pdf_url": "https://arxiv.org/pdf/2008.02995v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2309.04015v3",
    "title": "Optimal Transport with Tempered Exponential Measures",
    "abstract": "In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, \"\\`a-la-Kantorovich\", which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, \"\\`a-la-Sinkhorn-Cuturi\", which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that an extension of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity, which is under control up to sparsity patterns. In addition, our formulation fits naturally in the unbalanced optimal transport problem setting.",
    "authors": [
      "Ehsan Amid",
      "Frank Nielsen",
      "Richard Nock",
      "Manfred K. Warmuth"
    ],
    "category": "cs.LG",
    "published_date": "2023-09-07 20:53:23+00:00",
    "updated_date": "2024-02-16 16:35:29+00:00",
    "arxiv_url": "http://arxiv.org/abs/2309.04015v3",
    "pdf_url": "https://arxiv.org/pdf/2309.04015v3.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2106.07501v2",
    "title": "Balanced Coarsening for Multilevel Hypergraph Partitioning via   Wasserstein Discrepancy",
    "abstract": "We propose a balanced coarsening scheme for multilevel hypergraph partitioning. In addition, an initial partitioning algorithm is designed to improve the quality of k-way hypergraph partitioning. By assigning vertex weights through the LPT algorithm, we generate a prior hypergraph under a relaxed balance constraint. With the prior hypergraph, we have defined the Wasserstein discrepancy to coordinate the optimal transport of coarsening process. And the optimal transport matrix is solved by Sinkhorn algorithm. Our coarsening scheme fully takes into account the minimization of connectivity metric (objective function). For the initial partitioning stage, we define a normalized cut function induced by Fiedler vector, which is theoretically proved to be a concave function. Thereby, a three-point algorithm is designed to find the best cut under the balance constraint.",
    "authors": [
      "Zhicheng Guo",
      "Jiaxuan Zhao",
      "Licheng Jiao",
      "Xu Liu"
    ],
    "category": "cs.LG",
    "published_date": "2021-06-14 15:30:34+00:00",
    "updated_date": "2023-07-13 05:51:09+00:00",
    "arxiv_url": "http://arxiv.org/abs/2106.07501v2",
    "pdf_url": "https://arxiv.org/pdf/2106.07501v2.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2111.06546v1",
    "title": "Approximating Optimal Transport via Low-rank and Sparse Factorization",
    "abstract": "Optimal transport (OT) naturally arises in a wide range of machine learning applications but may often become the computational bottleneck. Recently, one line of works propose to solve OT approximately by searching the \\emph{transport plan} in a low-rank subspace. However, the optimal transport plan is often not low-rank, which tends to yield large approximation errors. For example, when Monge's \\emph{transport map} exists, the transport plan is full rank. This paper concerns the computation of the OT distance with adequate accuracy and efficiency. A novel approximation for OT is proposed, in which the transport plan can be decomposed into the sum of a low-rank matrix and a sparse one. We theoretically analyze the approximation error. An augmented Lagrangian method is then designed to efficiently calculate the transport plan.",
    "authors": [
      "Weijie Liu",
      "Chao Zhang",
      "Nenggan Zheng",
      "Hui Qian"
    ],
    "category": "cs.LG",
    "published_date": "2021-11-12 03:10:45+00:00",
    "updated_date": "2021-11-12 03:10:45+00:00",
    "arxiv_url": "http://arxiv.org/abs/2111.06546v1",
    "pdf_url": "https://arxiv.org/pdf/2111.06546v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2303.13971v1",
    "title": "Optimal Transport for Offline Imitation Learning",
    "abstract": "With the advent of large datasets, offline reinforcement learning (RL) is a promising framework for learning good decision-making policies without the need to interact with the real environment. However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive. In this paper, we introduce Optimal Transport Reward labeling (OTR), an algorithm that assigns rewards to offline trajectories, with a few high-quality demonstrations. OTR's key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we show that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards.",
    "authors": [
      "Yicheng Luo",
      "Zhengyao Jiang",
      "Samuel Cohen",
      "Edward Grefenstette",
      "Marc Peter Deisenroth"
    ],
    "category": "cs.LG",
    "published_date": "2023-03-24 12:45:42+00:00",
    "updated_date": "2023-03-24 12:45:42+00:00",
    "arxiv_url": "http://arxiv.org/abs/2303.13971v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13971v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  },
  {
    "id": "2306.00620v1",
    "title": "OTW: Optimal Transport Warping for Time Series",
    "abstract": "Dynamic Time Warping (DTW) has become the pragmatic choice for measuring distance between time series. However, it suffers from unavoidable quadratic time complexity when the optimal alignment matrix needs to be computed exactly. This hinders its use in deep learning architectures, where layers involving DTW computations cause severe bottlenecks. To alleviate these issues, we introduce a new metric for time series data based on the Optimal Transport (OT) framework, called Optimal Transport Warping (OTW). OTW enjoys linear time/space complexity, is differentiable and can be parallelized. OTW enjoys a moderate sensitivity to time and shape distortions, making it ideal for time series. We show the efficacy and efficiency of OTW on 1-Nearest Neighbor Classification and Hierarchical Clustering, as well as in the case of using OTW instead of DTW in Deep Learning architectures.",
    "authors": [
      "Fabian Latorre",
      "Chenghao Liu",
      "Doyen Sahoo",
      "Steven C. H. Hoi"
    ],
    "category": "cs.LG",
    "published_date": "2023-06-01 12:45:00+00:00",
    "updated_date": "2023-06-01 12:45:00+00:00",
    "arxiv_url": "http://arxiv.org/abs/2306.00620v1",
    "pdf_url": "https://arxiv.org/pdf/2306.00620v1.pdf",
    "doi": null,
    "journal_ref": null,
    "topic_category": "optimal_transport"
  }
]